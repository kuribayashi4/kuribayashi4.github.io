<!doctype html>
<html lang="">

<head>
    <meta charset="utf-8" />
    <title>Tatsuki Kuribayashi</title>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-92GGQSGY3S"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-92GGQSGY3S');
    </script>

    <meta name="author" content="Tatsuki Kuribayashi">
    <link rel="top" href="#" />
    <link
        href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600,300italic,400italic,600italic|Source+Code+Pro'
        rel='stylesheet' type='text/css'>
    </link>
    <link rel="stylesheet" href="kuribayashi4.github.io/theme/css/main.css" type="text/css" />
    <link href="kuribayashi4.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
        title="Tatsuki Kuribayashi Atom Feed" />
</head>

<body>
    <div class="container">
        <div class="wrapper">
            <div role="main" class="content">
                <article style="padding: 2%">
                    <h1>Tatsuki Kuribayashi (栗林樹生)</h1>
                    <table>
                        <tr style="align-items: left;">
                          <td style="width: 150px; display: inline-block">
                            <img src="./images/face4_2024.png" alt="写真" title="profile" width="145" height="200">
                          </td>
                          <td style="width: 60%; display: inline-block">
                            <ul>
                              <li>Postdoc in <a href="https://mbzuai.ac.ae/research/department/natural-language-processing-department/">MBZUAI</a>. Previously, I was in <a href="https://www.nlp.ecei.tohoku.ac.jp/">Tohoku NLP Group</a>.</li>
                              <li>Email: Tatsuki.Kuribayashi at mbzuai.ac.ae (kuribayashi at tohoku.ac.jp does not work currently)</li>
                              <li><a href="https://scholar.google.co.jp/citations?user=-bqmkaAAAAAJ&hl=ja">Google Scholar</a></li>
                              <li><a href="https://twitter.com/ttk_kuribayashi?lang=en">Twitter</a></li>
                              <li><a href="https://github.com/kuribayashi4">Github</a></li>
                              <li><a href="https://docs.google.com/document/d/1EyIgvUHRHemEshNcW6z2uZyPTyh5bzS_UJl_HnN1U7Q/edit?usp=sharing">CV</a></li>
                            </ul>
                            <div style="color: gray; font-size: small; margin-left: 30px;">
                              My research interest lies in leveraging NLP techniques to understand humans and language. I especially focus on finding cognitive biases in human language processing behavior and language designs, analyzing NLP models from linguistic and/or neuro-symbolic perspectives, and developing a system to help our language activities (especially from the standpoint of a non-native English learner).
                            </div>
                          </td>
                        </tr>
                      </table>
                    <h2>News</h2>
                    <ul>
                    <li>2024/08: Workshop on Cognitive Modeling and Computational Linguistics (CMCL) 2024 will be held in ACL 2024, Thailand. Organizers: Tatsuki Kuribayashi, Giulia Rambelli, Ece Takmaz, Philipp Wicke, Yohei Oseki. [<a href="https://www.aclweb.org/portal/content/cmcl-2024-13th-edition-workshop-cognitive-modeling-and-computational-linguistics">cfp</a>][<a href="https://cmclorg.github.io/">web</a>]</li>
                    <li>2024/05: I gave a talk at ETH Zürich [<a href="https://speakerdeck.com/kuribayashi4/from-cognitive-modeling-to-typological-universals-investigations-with-large-language-models">slides</a>]</li>
                    <li>2024/03: I gave a talk at The Hong Kong Polytechnic University (online) [<a href="https://speakerdeck.com/kuribayashi4/cognitive-im-plausibility-of-large-language-models">slides</a>]</li>
                    <li>2024/03: I serve an action editor in ACL Rolling Review (Interpretability and Analysis of Models for NLP / Linguistic theories, Cognitive Modeling and Psycholinguistics).
                    <li>2024/02: Selected as a <a href="https://aclrollingreview.org/great-oct23-reviewers/">great reviewer</a>. ACL Rolling Review (Linguistic theories, Cognitive Modeling and Psycholinguistics), 2023 October.</li>
                    <li>2023/04: I join MBZUAI as a postdoctoral researcher.</li></li>
                    </ul>
                    <h2>Publications</h2>
                    <h3>Refereed papers</h3>
                    <ul>
                        <li><u>Tatsuki Kuribayashi</u>, Ryo Ueda, Ryo Yoshida, Yohei Oseki, Ted Briscoe, Timothy Baldwin.<br>
                            "Emergent Word Order Universals from Cognitively-Motivated Language Models." <br>
                            The 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024, main long), 2024/08.<br>
                            [<a href="https://arxiv.org/abs/2402.12363">arXiv</a>]</li>
                        <li><u>Tatsuki Kuribayashi</u>, Yohei Oseki, Timothy Baldwin.<br>
                            "Psychometric Predictive Power of Large Language Models." <br>
                            Findings of the 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2024, Findings long), 2024/06. (acceptance rate: top 869/2434=35.7%)<br>
                            [<a href="https://arxiv.org/abs/2311.07484">arXiv</a>]</li>
                        <li>
                            大羽未悠, <u>栗林樹生</u>, 大内啓樹, 渡辺太郎. <br>
                             言語モデルの第二言語獲得. <br>
                             自然言語処理 (Japan domestic journal), Volume 31, Number 2, pp.433-455, 2024/06.
                        </li>
                        <li>Yukiko Ishizuki, <u>Tatsuki Kuribayashi</u>, Yuichiroh Matsubayashi, Ryohei Sasano and Kentaro Inui.<br>
                            "To Drop or Not to Drop? Predicting Argument Ellipsis Judgments: A Case Study in Japanese."<br>
                            In Proceedings of The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024, long), 2024/05. (acceptance rate: 1556/3417=52%)<br>
                            [<a href="https://arxiv.org/abs/2404.11315">arXiv</a>]</li>
                        </li>
                        <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                            "Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps."<br>
                            In Proceedings of 12th International Conference on Learning Representations (ICLR 2024, spotlight, top 5%), 2024/05. (acceptance rate: 2260/7262=31%)<br>[<a
                                href="https://arxiv.org/abs/2302.00456">arXiv</a>]</li>
                        <li>Mengyu Ye, <u>Tatsuki Kuribayashi</u>, Jun Suzuki, Hiroaki Funayama and Goro Kobayashi.<br>
                            "Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism."<br>
                            In Proceedings of The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP-2023, main short), 2023/12. (acceptance rate: 146/1041=14.0%)<br>
                            [<a href="https://aclanthology.org/2023.emnlp-main.912/">paper</a> | <a href="https://arxiv.org/abs/2310.14868v1">arXiv</a>]
                        <ul>
                            <li>Mengyu Ye, <u>Tatsuki Kuribayashi</u>, Jun Suzuki, Hiroaki Funayama and Goro Kobayashi.<br>
                            "Assessing Chain-of-Thought Reasoning against Lexical Negation: A Case Study on Syllogism."<br>
                            In Proceedings of Student Research Workshop (SRW) at the 61st Annual Meeting of the Association for Computational Linguistics 2023 (ACL-SRW, Non-archival, <b>best paper award</b>), 2023/07.<br>
                        </ul>
                        </li>
                        <li>
                            Takumi Ito, Naomi Yamashita, <u>Tatsuki Kuribayashi</u>, Masatoshi Hidaka, Jun Suzuki, Ge Gao, Jack Jamieson and Kentaro Inui.<br>
                            "Use of an AI-powered Rewriting Support Software in Context with Other Tools: A Study of Non-Native English Speakers."<br>
                            The ACM Symposium on User Interface Software and Technology 2023 (UIST 2023), 2023/10.<br>
                            [<a href="https://dl.acm.org/doi/abs/10.1145/3586183.3606810">paper</a>]
                        </li>
                        <li>Miyu Oba, <u>Tatsuki Kuribayashi</u>, Hiroki Ouchi, Taro Watanabe. <br>
                            "Second Language Acquisition of Neural Language Models."<br>
                            Findings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL-2023, Findings long), 2023/07. (acceptance rate: top 39.1%)<br>
                            [<a href="https://aclanthology.org/2023.findings-acl.856/">paper</a> | <a href="https://arxiv.org/abs/2306.02920">arXiv</a>]
                        </li>
                        <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi and Kentaro Inui. <br>
                            "Transformer Language Models Handle Word Frequency in Prediction Head." <br>
                            Findings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL-2023, Findings short), 2023/07. (acceptance rate: top 39.1%)<br>
                            [<a href="https://aclanthology.org/2023.findings-acl.276/">paper</a> | <a href="https://arxiv.org/abs/2305.18294">arXiv</a>]
                        </li>
                        <li>Keito Kudo, Yoichi Aoki, <u>Tatsuki Kuribayashi</u>, Ana Brassard, Masashi Yoshikawa, Keisuke Sakaguchi and Kentaro Inui. <br>
                            "Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?" <br>
                            Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2023, main short), 2023/05. (acceptance rate: 281/1166=24.1%)<br>
                            [<a href="https://aclanthology.org/2023.eacl-main.98/">paper</a> | <a href="https://arxiv.org/abs/2302.07866">arXiv</a>]
                        </li>
                        <li>Yoichi Aoki, Keito Kudo, <u>Tatsuki Kuribayashi</u>, Ana Brassard, Masashi Yoshikawa, Keisuke Sakaguchi and Kentaro Inui. <br>
                            "Empirical Investigation of Neural Symbolic Reasoning Strategies." <br>
                            Findings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2023, Findings short), 2023/05. (acceptance rate: top 482/1166=41.3%)<br>
                            [<a href="https://aclanthology.org/2023.findings-eacl.86/">paper</a> | <a href="https://arxiv.org/abs/2302.08148">arXiv</a>]
                        </li>
                        <ul>
                            <li>Yoichi Aoki, Keito Kudo, <u>Tatsuki Kuribayashi</u>, Ana Brassard, Masashi Yoshikawa, Keisuke Sakaguchi and Kentaro Inui. <br>
                                "Empirical Investigation of Neural Symbolic Reasoning Strategies." <br>
                                Non-archival submission for the 2022 AACL-IJCNLP Student Research Workshop (AACL-IJCNLP SRW Non-archival, <b>best paper award</b>), 2022/11.</li>
                        </ul>
                        <li><u>Tatsuki Kuribayashi</u>, Yohei Oseki, Ana Brassard, Kentaro Inui.<br>
                            "Context Limitations Make Neural Language Models More Human-Like."<br>
                            In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP-2022, main long), pp.10421-10436, 2022/12. (acceptance rate: 829/4190=20%)<br>
                            [<a href="https://aclanthology.org/2022.emnlp-main.712/">paper</a> | <a href="https://arxiv.org/abs/2205.11463">arXiv</a>]
                        </li>
                        <li>
                            Riki Fujihara, <u>Tatsuki Kuribayashi</u>, Kaori Abe, Ryoko Tokuhisa, Kentaro Inui.<br>
                            "Topicalization in Language Models: A Case Study on Japanese."<br> 
                            In Proceedings of the 29th International Conference on Computational Linguistics (COLING-2022, long), pp.851-862, 2022/10 (acceptance rate: 522/1563=33.4%).<br>
                            [<a href="https://aclanthology.org/2022.coling-1.71/">paper</a>]
                        </li>
                        <ul>
                            <li>Riki Fujihara, <u>Tatsuki Kuribayashi</u>, Kaori Abe, Kentaro Inui.<br>
                                "Topicalization in Language Models: A Case Study on Japanese."<br>
                                In proceedings of Student Research Workshop at the Joint Conference of the 59th Annual
                                Meeting of the Association for Computational Linguistics and the 11th International Joint
                                Conference on Natural Language Processing (ACL-IJCNLP-2021 SRW Non-archival), 2021/08
                                (acceptance rate: 39%).<br>
                            </li>
                        </ul>
                        <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                            "Incorporating Residual and Normalization Layers into Analysis of Masked Language
                            Models."<br> In proceedings of the 2021 Conference on Empirical Methods in Natural Language
                            Processing (EMNLP 2021, main long), pp. 4547-4568, 2021/11 (acceptance rate: 23.3%).<br>
                            [<a href="https://aclanthology.org/2021.emnlp-main.373/">paper</a> | <a href="https://arxiv.org/abs/2109.07152">arXiv</a> | code]
                        </li>
                        <li>Hiroki Ouchi, Jun Suzuki, Sosuke Kobayashi, Sho Yokoi, <u>Tatsuki Kuribayashi</u>, Masashi Yoshikawa, Kentaro Inui.<br>
                            "Instance-Based Neural Dependency Parsing."<br> Transactions of the Association for Computational Linguistics 2021 (TACL 2021), 2021/09.<br>
                            [<a href="https://aclanthology.org/2021.tacl-1.89/">paper</a> | <a href="https://arxiv.org/abs/2109.13497">arXiv</a>]
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Yohei Oseki, Takumi Ito, Ryo Yoshida, Masayuki Asahara,
                            Kentaro Inui.<br>
                            "Lower Perplexity is Not Always Human-Like."<br>
                            In proceedings of the Joint Conference of the 59th Annual Meeting of the Association for
                            Computational Linguistics and the 11th International Joint Conference on Natural Language
                            Processing (ACL-IJCNLP 2021, main long), pp. 5203-5217, 2021/08 (acceptance rate: 21.3%).<br>
                            [<a href="https://aclanthology.org/2021.acl-long.405/">paper</a> | <a
                                href="https://github.com/kuribayashi4/surprisal_reading_time_en_ja">code</a>]
                        </li>
                        <li><u>栗林樹生</u>, 大内啓樹, 井之上直也, 鈴木潤, Paul Reisert, 三好利昇, 乾健太郎<br>
                            「論述構造解析におけるスパン分散表現」<br>
                            自然言語処理 (domestic journal), Volume 27, Number 4, pp.753-780, 2020/12.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jnlp/27/4/27_753/_article/-char/ja/">paper</a> |
                            <a href="./pdfs/20210308_Argmining_kuribayashi_public.pdf">slides</a>]
                        </li>
                        <li>Takaki Otake, Sho Yokoi, Naoya Inoue, Ryo Takahashi, <u>Tatsuki Kuribayashi</u>, Kentaro
                            Inui.<br>
                            "Modeling Event Salience in a Narrative Based on Barthes' Cardinal Function."<br>
                            In proceedings of the 28th International Conference on Computational Linguistics
                            (COLING-2020, short), pp. 1784-1794, 2020/12 (acceptance rate: 26.2%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.coling-main.160/">paper</a> | <a
                                href="https://arxiv.org/abs/2011.01785">arXiv</a>]
                        </li>
                        <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                            "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms."<br>
                            In proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
                            (EMNLP-2020, main long), pp. 7057-7075, 2020/11 (acceptance rate: 754/3359=22.4%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.emnlp-main.574/">paper</a> | <a
                                href="https://arxiv.org/abs/2004.10102">arXiv</a> | <a
                                href="https://github.com/gorokoba560/norm-analysis-of-transformer">code</a>]
                        </li>
                        <ul>
                            <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                                "Self-Attention is Not Only a Weight: Analyzing BERT with Vector Norms."<br>
                                In proceedings of Student Research Workshop at the 58th Annual Meeting of the Association
                                for Computational Linguistics (ACL-SRW-2020 Non-archival), 2020/07 (acceptance rate:
                                72/202=35.6%).<br>
                            </li>
                        </ul>
                        <li>*Takumi Ito, *<u>Tatsuki Kuribayashi</u>, *Masatoshi Hidaka, Jun Suzuki, Kentaro Inui. (*
                            equal contribution)<br>
                            "Langsmith: An Interactive Academic Text Revision System."<br>
                            In proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
                            (EMNLP-2020, demo), pp. 216-226, 2020/11 (acceptance
                            rate: ???%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.emnlp-demos.28/">paper</a> | <a
                                href="https://arxiv.org/abs/2010.04332">arXiv</a> | <a
                                href="https://emnlp-demo.editor.langsmith.co.jp/">demo</a>]
                        </li>
                        <li>Takumi Ito, <u>Tatsuki Kuribayashi</u>, Hayato Kobayashi, Ana Brassard, Masato Hagiwara, Jun
                            Suzuki, Kentaro Inui.<br>
                            "Assisting Authors to Convert Raw Products into Polished Prose."<br>
                            Journal of Cognitive Science, Vol.21, No.1, pp.99-135, 2020.<br>
                            [paper]</li>
                        <li><u>Tatsuki Kuribayashi</u>, Takumi Ito, Jun Suzuki, Kentaro Inui.<br>
                            "Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in
                            Japanese."<br>
                            In proceedings of the 58th annual meeting of the Association for Computational Linguistics
                            (ACL-2020, main long), pp. 6452-6459, 2020/07 (acceptance rate: 779/3429=22.7%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.acl-main.47/">paper</a> | <a
                                href="./pdfs/revised_20211223.pdf">updated version</a> | <a
                                href="https://github.com/kuribayashi4/LM_as_Word_Order_Evaluator">data</a> | <a
                                href="./pdfs/20200606_ACL2020.pptx">presentation source</a>]
                        </li>
                        <li>Hiroki Ouchi, Jun Suzuki, Sosuke Kobayashi, Sho Yokoi, <u>Tatsuki Kuribayashi</u>, Ryuto
                            Konno, Kentaro Inui.<br>
                            "Instance-Based Learning of Span Representations: A Case Study through Named Entity
                            Recognition."<br>
                            In proceedings of the 58th annual meeting of the Association for Computational Linguistics
                            (ACL-2020, Short), pp. 6452-6459, 2020/07 (acceptance rate: 208/1185=17.6%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.acl-main.575/">paper</a> | <a
                                href="https://arxiv.org/abs/2004.14514">arXiv</a>]
                        </li>
                        <li>*Takumi Ito, *<u>Tatsuki Kuribayashi</u>, Hayato Kobayashi, Ana Brassard, Masato Hagiwara,
                            Jun Suzuki, Kentaro Inui. (* equal contribution)<br>
                            "Diamonds in the Rough: Generating Fluent Sentences from Early-stage Drafts for Academic
                            Writing Assistance." <br>
                            In Proceedings of the 12th International Conference on Natural Language Generation
                            (INLG-2019), pp. 40-53, 2019/10
                            (acceptance rate: 73/143=51.0%).<br>[<a
                                href="https://www.aclweb.org/anthology/W19-8606/">paper</a> | <a
                                href="https://arxiv.org/abs/1910.09180">arXiv</a> | <a
                                href="https://github.com/taku-ito/INLG2019_SentRev">data</a>]</li>

                        <li>Masato Hagiwara, Takumi Ito, <u>Tatsuki Kuribayashi</u>, Jun Suzuki, Kentaro Inui.<br>
                            "TEASPN: Framework and Protocol for Integrated Writing Assistance Environments." <br>
                            In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
                            (EMNLP-IJCNLP 2019, demo), pp. 229-234, 2019/11 
                            (acceptance rate: ???%).<br>[<a href="https://www.aclweb.org/anthology/D19-3039">paper</a>|
                            <a href="https://arxiv.org/abs/1909.02621">arXiv</a> | <a
                                href="https://github.com/teaspn/teaspn-sdk">code</a>]
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Hiroki Ouchi, Naoya Inoue, Paul Reisert, Toshinori Miyoshi, Jun
                            Suzuki, Kentaro Inui.<br>
                            "An Empirical Study of Span Representations in Argumentation Structure Parsing." <br>
                            In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics
                            (ACL-2019, short), pp. 4691-4698, 2019/07 
                            (acceptance rate: 213/1163=18.2%).<br>[<a
                                href="https://www.aclweb.org/anthology/P19-1464">paper</a> | <a
                                href="https://github.com/kuribayashi4/span_based_argumentation_parser">code</a>]</li>

                        <li>Paul Reisert, Naoya Inoue, <u>Tatsuki Kuribayashi</u>, Kentaro Inui.<br>
                            "Feasible Annotation Scheme for Capturing Policy Argument Reasoning using Argument
                            Templates." <br>
                            In Proceedings of the 5th Workshop on Argument Mining, pp.79-89, 2018/11 
                            (acceptance rate: 18/32=56.3%).<br>[<a
                                href="https://www.aclweb.org/anthology/W18-5210">paper</a> | <a
                                href="https://github.com/peldszus/arg-microtexts">data</a>]</li>
                    </ul>
                    <h3>Preprints</h3>
                    <ul>
                        <li>
                            David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo,...,<u>Tatsuki Kuribayashi</u>,...,Thamar Solorio, Alham Fikri Aji<br>
                            "CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark."<br>
                            [<a href="https://arxiv.org/abs/2406.05967">arXiv</a>]
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>.<br>
                        "Does Vision Accelerate Hierarchical Generalization of Neural Language Learners?" <br>[<a
                            href="https://arxiv.org/abs/2302.00667">arXiv</a>]</li>
                    </ul>
                    <h2>Awards</h2>
                    <ul>
                        <li><a href="https://www.anlp.jp/nlp2024/award.html">2024/03: Outstanding Paper Award, NLP2024 (domestic conference), 言語処理学会第30回年次大会, 優秀賞</a>. どのような言語モデルが不可能な言語を学習してしまうのか？―語順普遍を例に―</li>
                        <li><a href="https://www.anlp.jp/nlp2024/award.html">2024/03: Special Committee Award, NLP2024 (domestic conference), 言語処理学会第30回年次大会, 委員特別賞</a>. 長距離相互作用する文脈依存言語における相転移現象 ―言語モデルの創発現象を統計力学の視点で理解する―</li>
                        <li><a href="https://2023.aclweb.org/program/best_papers/">2023/07: ACL-SRW Best Paper Award</a>. Assessing Chain-of-Thought Reasoning against Lexical Negation: A Case Study on Syllogism.</li>
                        <li><a href="https://www.anlp.jp/nlp2023/award.html">2023/03: Special Committee Award, NLP2023 (domestic conference). 言語処理学会第29回年次大会, 委員特別賞</a>. 百聞は一見に如かず？視覚情報は言語モデルに文の階層構造を教示するか.</li>
                        <li><a href="https://www.anlp.jp/nlp2023/award.html">2023/03: Special Committee Award, NLP2023 (domestic conference). 言語処理学会第29回年次大会, 委員特別賞</a>. 日本語話者の項省略判断に関するアノテーションとモデリング.</li>
                        <li><a href="https://nl-ipsj.or.jp/award/">2022/12: Excellent Research Award, 第254回 NL研 (domestic conference)</a>. 言語モデルの第二言語獲得効率.</li>
                        <li><a href="https://aacl2022-srw.github.io/program">2022/11: AACL-SRW Best Paper Award</a>. Empirical Investigation of Neural Symbolic Reasoning Strategies.</li>
                        <li><a href="https://yans.anlp.jp/entry/yans2022report">2022/08: Encouragement Awards (奨励賞), YANS2022 (domestic meeting).</a> 視覚情報は言語モデルに人間らしい統語的汎化を促すか.</li>
                        <li>2022/03: President's Award, Graduate School of Information Sciences, Tohoku University. (東北大学総長賞)</li>
                        <li><a href="https://note.com/ipsj/n/nefc2ae945988">2021年度研究会推薦博士論文</a>. Exploring Cognitive Plausibility of Neural NLP Models: Cross-Linguistic and Discourse-Level Studies.</li>
                        <li>2022/03: Excellent
                            Student in Electrical and Information Science Award, Tohoku University. (東北大学電気・情報系優秀学生賞)</li>
                        <li><a href="https://www.anlp.jp/nlp2022/award.html">2022/03: Special Committee Award, NLP2022 (domestic conference). (言語処理学会第28回年次大会, 委員特別賞) </a>Transformerにおけるフィードフォワードネットの作用</li>
                        <li><a href="https://www.anlp.jp/award/ronbun.html">2021/03: Best Paper Award 2020, Association for Natural Language
                                Processing (domestic journal). (言語処理学会2020年度最優秀論文賞) (1/28=3.5%)</a>
                            論述構造解析におけるスパン分散表現<br>[<a
                                href="pdfs/20210308_Argmining_kuribayashi_public.pdf">年次大会招待講演資料</a>]</li>
                        <li><a href="https://www.anlp.jp/nlp2021/">2021/03: Special Committee Award, NLP2021
                                (domestic conference). (言語処理学会第27回年次大会, 委員特別賞) (14/361=3.8%)</a>
                            予測の正確な言語モデルがヒトらしいとは限らない</li>
                        <li><a href="https://www.nlp.ecei.tohoku.ac.jp/news-release/3571/">2020/03: Excellent
                                Student in Electrical and Information Science Award, Tohoku University. (東北大学電気・情報系優秀学生賞) </a></li>
                        <li><a href="https://www.anlp.jp/award/nenji.html">2020/03: Best Paper Award, NLP2020
                                (domestic conference) (言語処理学会年次大会最優秀賞) (2/396=0.5%)</a> ベクトル⻑に基づく自己注意機構の解析. </li>
                        <li><a href="https://www.anlp.jp/award/nenji.html#y2019">2019/03: Young Researcher Award, 
                                NLP2019 (domestic conference). (言語処理学会年次大会若手奨励賞) (11/274=4.0%)</a>
                            言語モデルを用いた日本語の語順評価と基本語順の分析.</li>
                        <li><a href="http://www.ecei.tohoku.ac.jp/eipe/intro/news/commendation.html">2018/03: Excellent
                                Academic Award, School of Engineering, Tohoku University. (東北大学工学部長賞) (26/910=2.9%)</a>
                        </li>
                    </ul>
                    <h2>Grant</h2>
                    <ul>
                        <li>2023/04-2026/03 (suspended due to overseas affiliation): Grant-in-Aid for Early-Career Scientists, 科学研究費助成事業若手研究. 「人間らしい言語獲得効率に対するマルチモーダル言語処理を通した構成論的探求」</li>
                        <li>2020/04-2022/03: Doctoral Course (DC) Research Fellowships, 日本学術振興会特別研究員(DC1) (面接免除内定, 54/277=19.5%).
                            「テクストの数理的モデリングと、数理モデルを通したテクストらしさの解明への挑戦」</li>
                    </ul>

                    <h2>Education/affiliation</h2>
                    <ul> 
                        <li>2023/04-: Postdoctoral research fellow in <a href="https://mbzuai.ac.ae/research/department/natural-language-processing-department/">MBZUAI</a> (Adviser: Prof. <a href="https://people.eng.unimelb.edu.au/tbaldwin/">Timothy Baldwin</a>).
                        </li>
                        <li>2022/04-2023/03: Specially-appointed research fellow in <a href="https://www.nlp.ecei.tohoku.ac.jp/">Tohoku NLP Group</a>
                        </li>
                        <li>2020/04-2022/03: PhD student of Information Science, Graduate School of Information
                            Sciences, Tohoku University, Miyagi, Japan. (Superviser: <a
                                href="http://www.cl.ecei.tohoku.ac.jp/~inui/">Prof. Kentaro Inui</a>, thesis title: Exploring Cognitive Plausibility of Neural NLP Models: Cross-Linguistic and Discourse-Level Studies.) <br>
                                [<a href="pdfs/2024_download_PhD_honshinsa_Thesis_Kuribayashi_.pdf">thesis</a>]</li>
                        <li>2018/04-2020/03: Master's student of Information Science, Graduate School of Information
                            Sciences, Tohoku University, Miyagi, Japan. (Superviser: <a
                                href="http://www.cl.ecei.tohoku.ac.jp/~inui/">Prof. Kentaro Inui</a>) <br>
                            [<a href="http://www.cl.ecei.tohoku.ac.jp/local/handouts/2019/thesis/thesis-20200401-kuribayashi.pdf">thesis</a>]</li>
                        <li>2018/04-2022/03.: Graduate Program in Data Science, Tohoku University.</li>
                        <li>2014/04-2018/03: Bachelor of Engineering, Department of Information and Intelligent
                            Systems, Tohoku University, Miyagi, Japan. (Superviser: <a
                                href="http://www.cl.ecei.tohoku.ac.jp/~inui/">Prof. Kentaro Inui</a>) </li>
                    </ul>
                    <h2>Activities/organizer</h2>
                    <ul>
                        <li>2018/05-2023/03: <a href="https://langsmith.co.jp/">Langsmith Co., Ltd.</a> (Co-founder).
                        </li>
                        <li>2020-2023: <a href="https://sites.google.com/view/snlp-jp">最先端NLP勉強会</a> (2023実行委員長)
                        </li>
                        <li>2021-2022: Member of NLP Dの会, 2021-2022.</li>
                        <li>2019/08: <a href="https://www.uni-goettingen.de/en/data+science/594011.html">3rd Data
                            Science Summer School in Göttingen</a>.</li>
                        <li>2017/08: <a href="https://internship.cookpad.com/2017/summer/17day-tech/">Cookpad 17days
                                Internship</a>.</li>
                    </ul>
                    <h2>Talks/writings</h2>
                    <ul>
                        <li>
                            <a href="https://sites.google.com/view/snlp-jp/home/2022">最先端NLP勉強会</a>, 2023/08.
                            [<a
                                href="https://speakerdeck.com/kuribayashi4/zui-xian-duan-nlplun-wen-shao-jie-a-watermark-for-large-language-models">slides</a>]
                                                         [<a
                                href="https://speakerdeck.com/kuribayashi4/zui-xian-duan-nlplun-wen-shao-jie-why-does-surprisal-from-larger-transformer-based-language-models-provide-a-poorer-fit-to-human-reading-times">slides</a>]
                        </li>
                        <li>自然言語処理による論文執筆支援. 東北大学（大学院改革推進センター主催）, 2023/2.
                            [<a href="https://speakerdeck.com/kuribayashi4/zi-ran-yan-yu-chu-li-niyorulun-wen-zhi-bi-zhi-yuan">slides</a>]
                        </li>
                        <li>
                            Talk at <a href="https://mbzuai.ac.ae/research/department/natural-language-processing-department/">MBZUAI</a>. "Cognitive Plausibility of Neual Language Models," 2022/12.
                          [<a href="https://speakerdeck.com/kuribayashi4/cognitive-plausibility-of-neural-language-models">slides</a>]
                        </li>
                        <li>
                            <a href="https://sites.google.com/view/snlp-jp/home/2022">最先端NLP勉強会</a>, 2022/09.
                            [<a
                                href="https://speakerdeck.com/kuribayashi4/zui-xian-duan-nlplun-wen-shao-jie-revisiting-the-uniform-information-density-hypothesis-emnlp2021-linguistic-dependencies-and-statistical-dependence-emnlp2021">slides</a>]
                        </li>
                        <li>
                            <a href="https://sites.google.com/view/snlp-jp/home/2021">最先端NLP勉強会</a>, 2021/09.
                            [<a
                                href="./pdfs/snlp2021.pdf">slides</a>]
                        </li>
                        <li><u>栗林樹生</u>. Lower Perplexity is Not Always Human-Like (Talk). NLPコロキウム, 2021/06. [<a
                            href="https://youtu.be/Xd_KfgWVWsI">動画</a>]</li>
                        <li><u>栗林樹生</u>. 学会記事「論述構造解析におけるスパン分散表現」の研究を通して. 自然言語処理 (学会記事No.27), Volume 28, Number 2,
                                pp.677-681, 2021/06.</li>
                        <li><u>栗林樹生</u>. 学会記事 Language Models as an Alternative Evaluator of Word Order Hypotheses: A
                                    Case Study in Japanese. 自然言語処理 (学会記事), Volume 27, Number 3, pp.671-676, 2020/09.
                         </li>
                        <li>
                            <a href="https://sites.google.com/view/snlp-jp/home/2020">最先端NLP勉強会</a>, 2020/09.
                            [<a
                                href="./pdfs/snlp2020.pdf">slides</a>]
                        </li>
                    </ul>
                    <h2>Tools</h2>
                    <ul>
                        <li>Langsmith Editor<br>[<a href="https://editor.langsmith.co.jp/">system</a> | <a
                                href="https://help.editor.langsmith.co.jp/">usage</a>] </li>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/xgzxLpcydG8" frameborder="0"
                            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen></iframe>
                        <li>TEASPN (Text Editing Assistance Smartness Protocol for Natural Language)<br>[<a
                                href="https://github.com/teaspn/teaspn-sdk">code</a> | <a
                                href="https://www.teaspn.org/">document</a>] </li>
                      <iframe width="560" height="315" src="https://www.youtube.com/embed/9DNqpudDehM" frameborder="0"
                            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen></iframe>
                    </ul>
                    <h2>Reviewer</h2>
                    <ul>
                        <li>*CL, EMNLP top conferences: 2021-present
                            <ul>
                            <li>ACL Rolling Review. Reviewer and Action Editor (Interpretability and Analysis of Models for NLP / Linguistic theories, Cognitive Modeling and Psycholinguistics)</li>
                            <li>ACL: 2019 (secondary), 2020 (secondary), 2021 (Sentiment Analysis, Stylistic Analysis, and Argument Mining), 2023 (Linguistic Theories, Cognitive Modeling and Psycholinguistics.)</li>
                            <li>EMNLP: 2021 (Linguistic Theories, Cognitive Modeling and Psycholinguistics. Sentiment
                                Analysis, Stylistic Analysis, and Argument Mining), 2022 (Linguistic Theories, Cognitive Modeling and Psycholinguistics).</li>
                            </ul>
                        </li>
                        <li>COLING: 2020, 2022, 2024 (Language Modeling. Integrated Systems and Applications).</li>
                        <li>LREC: 2022, 2024 (Language Resources and Evaluation for Psycholinguistics, Cognitive Linguistics and Linguistic Theories. Natural Language Generation (including Summarization)).</li>
                        <li>INLG: 2020, 2021.</li>
                        <li>COLM: 2024</li>
                        <li>CoNLL: 2024</li>
                        <li>BlackboxNLP: 2024</li>
                        <li>言語処理学会年次大会 (domestic conference), 学会誌自然言語処理.</li>
                    </ul>
                    <h2>Domestic conferences</h3>
                    <ul>
                        <li><u>栗林樹生</u>, 大関洋平, Timothy Baldwin. 大規模言語モデルの文処理は人間らしいのか？. 言語処理学会第30回年次大会,4pages,2024/3.</li>
                        <li><u>栗林樹生</u>, 上田亮, 吉田遼, 大関洋平, Ted Briscoe, Timothy Baldwin. どのような言語モデルが不可能な言語を学習してしまうのか？---語順普遍を例に---. 言語処理学会第30回年次大会,4pages,2024/3.</li>
                        <li>青木洋一, 工藤慧音, 曾根周作, <u>栗林樹生</u>, 谷口雅弥, 坂口慶祐, 乾健太郎. 言語モデルの思考連鎖的推論における探索戦略の動的変化. 言語処理学会第30回年次大会,4pages,2024/3.</li>
                        <li>工藤慧音, 青木洋一, <u>栗林樹生</u>, 谷口雅弥, 曾根周作, 坂口慶祐, 乾健太郎. 算術推論問題における自己回帰型言語モデルの内部機序. 言語処理学会第30回年次大会,4pages,2024/3.</li>
                        <li>葉夢宇, <u>栗林樹生</u>, 小林悟郎, 鈴木潤. 文脈内学習における文脈内事例の寄与度推定. 言語処理学会第30回年次大会,4pages,2024/3.</li>
                        <li>都地悠馬, 高橋惇, 横井祥, <u>栗林樹生</u>, 上田亮, 宮原英之. 長距離相互作用する文脈依存言語における相転移現象 -言語モデルの創発現象を統計力学の視点で理解する-. 言語処理学会第30回年次大会,4pages,2024/3.</li>
                        <li><u>栗林樹生</u>. 百聞は一見に如かず？視覚情報は言語モデルに文の階層構造を教示するか. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 乾健太郎. Transformer言語モデルの予測ヘッド内バイアスによる頻度補正効果. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>葉夢宇, <u>栗林樹生</u>, 舟山弘晃, 鈴木潤. 思考連鎖指示における大規模言語モデルの否定表現理解. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>藤原吏生, <u>栗林樹生</u>, 徳久良子, 乾健太郎. 主題化における人間と言語モデルの対照. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>石月由紀子, <u>栗林樹生</u>, 松林優一郎, 笹野遼平, 乾健太郎. 日本語話者の項省略判断に関するアノテーションとモデリング. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>大羽未悠, <u>栗林樹生</u>, 大内啓樹, 渡辺太郎. 言語モデルの第二言語獲得. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>工藤慧音, 青木洋一, <u>栗林樹生</u>, Ana Brassard, 吉川将司, 坂口慶祐, 乾健太郎. 算術問題におけるニューラルモデルの構成的推論能力. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>青木洋一, 工藤慧音, Ana Brassard, <u>栗林樹生</u>, 吉川将司, 坂口慶祐, 乾健太郎. ニューラル記号推論における推論過程の教示方法. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>大羽未悠, <u>栗林樹生</u>, 大内啓樹, 渡辺太郎. 言語モデルの第二言語獲得効率. 第254回自然言語処理研究会, 6pages, 2022/11.</li>
                        <li>青木洋一, 工藤慧音, <u>栗林樹生</u>, Ana Brassard, 吉川将司, 乾健太郎. 推論過程の性質がニューラルネットの多段推論能力に与える影響. 第17回NLP若手の会 シンポジウム (YANS), 2022/08.</li>
                        <li><u>栗林樹生</u>. 視覚情報は言語モデルに人間らしい統語的汎化を促すか. 第17回NLP若手の会 シンポジウム (YANS), 2022/08.</li>
                        <li>大羽未悠, <u>栗林樹生</u>, 大内啓樹, 渡辺太郎. 言語モデルの第二言語獲得効率. 第17回NLP若手の会 シンポジウム (YANS), 2022/08.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 乾健太郎. Transformerにおけるフィードフォワードネットの混ぜ合わせ作用. 第17回NLP若手の会 シンポジウム (YANS), 2022/08.</li>
                        <li><u>栗林樹生</u>, 大関洋平, Ana Brassard, 乾健太郎. ニューラル言語モデルの過剰な作業記憶. 言語処理学会第28回年次大会, pp.1530-1535, 2022/03.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 乾健太郎. Transformerにおけるフィードフォワードネットの作用. 言語処理学会第28回年次大会, pp.1072-1077, 2022/03.</li>
                        <li>石月由紀子, <u>栗林樹生</u>, 松林優一郎, 大関洋平. 情報量に基づく日本語項省略の分析. 言語処理学会第28回年次大会, pp.442-447, 2022/03.</li>
                        <li>青木洋一, 工藤慧音, Ana Brassard, <u>栗林樹生</u>, 吉川将司, 乾健太郎. 多段の数量推論タスクに対する適応的なモデルの振る舞いの検証. 言語処理学会第28回年次大会, pp.168-172, 2022/03.</li>
                        <li>石月由紀子, <u>栗林樹生</u>, 松林優一郎, 大関洋平 情報量に基づく日本語項省略の分析. NLP若手の会 (YANS) 第16回シンポジウム,
                            2021/08.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 乾健太郎. 非線形モジュールの加法分解に基づくTransformerレイヤーの解析. NLP若手の会 (YANS)
                            第16回シンポジウム, 2021/08.</li>
                        <li><u>栗林樹生</u>, 大関洋平, 伊藤拓海, 吉田遼, 浅原正幸, 乾健太郎. 予測の正確な言語モデルがヒトらしいとは限らない. 言語処理学会第27回年次大会,
                            pp.267-272,
                            2021/03.</li>
                        <li><u>栗林樹生</u>, 大関洋平, 伊藤拓海, 吉田遼, 浅原正幸, 乾健太郎. 日本語の読みやすさに対する情報量に基づいた統一的な解釈. 言語処理学会第27回年次大会,
                            pp.723-728, 2021/03.</li>
                        <li>伊藤拓海, <u>栗林樹生</u>, 日高雅俊, 鈴木潤, 乾健太郎. Langsmith: 人とシステムの協働による論文執筆. 言語処理学会第27回年次大会,
                            pp.1834-1839,
                            2021/03.</li>
                        <li>藤原吏生, <u>栗林樹生</u>, 乾健太郎. 人と言語モデルが捉える文の主題. 言語処理学会第27回年次大会, pp.1307-1312, 2021/03.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 乾健太郎. Transformerの文脈を混ぜる作用と混ぜない作用. 言語処理学会第27回年次大会,
                            pp.1224-1229,
                            2021/03.</li>
                        <li>大内啓樹, 鈴木潤, 小林颯介, 横井祥, <u>栗林樹生</u>, 吉川将司,
                            乾健太郎. 事例ベース依存構造解析のための依存関係表現学習. 言語処理学会第27回年次大会, pp.497-502, 2021/03.</li>
                        <li>大竹孝樹, 横井祥, 井之上直也, 高橋諒, <u>栗林樹生</u>,
                            乾健太郎. 物語におけるイベントの顕現性推定と物語類似性計算への応用. 言語処理学会第27回年次大会, pp.1324-1329, 2021/03.</li>
                        <li>藤原吏生, <u>栗林樹生</u>, 乾健太郎. 日本語言語モデルが選択する文形式の傾向と文脈の影響 ー 主題化・有標語順について. NLP若手の会 (YANS)
                            第15回シンポジウム, 2020/09.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 乾健太郎. ベクトル長に基づく注意機構と残差結合の包括的な分析. NLP若手の会 (YANS) 第15回シンポジウム,
                            2020/09.</li>
                        <li><u>栗林樹生</u>, 伊藤拓海, 鈴木潤, 乾健太郎. 日本語語順分析に言語モデルを用いることの妥当性について. 言語処理学会第26回年次大会,
                            pp.493-496,
                            2020/03.<br>[<a href="./pdfs/nlp2020.pdf">slides</a>]</li>
                        <li>小林 悟郎, <u>栗林樹生</u>, 横井 祥, 鈴木潤, 乾健太郎. ベクトル⻑に基づく自己注意機構の解析. 言語処理学会第26回年次大会, pp.965-968,
                            2020/03.</li>
                        <li>大内 啓樹, 鈴木 潤, 小林 颯介, 横井 祥, <u>栗林樹生</u>, 乾健太郎. スパン間の類似性に基づく事例ベース構造予測. 言語処理学会第26回年次大会,
                            pp.331-334, 2020/03.</li>
                        <li>大竹 孝樹, 横井 祥, 井之上 直也, 高橋 諒, <u>栗林樹生</u>, 乾健太郎. 言語モデルによる物語中のイベントの顕現性推定. 言語処理学会第26回年次大会,
                            pp.1089-1092, 2020/03.</li>
                        <li>伊藤拓海, <u>栗林樹生</u>, 萩原正人, 鈴木潤, 乾健太郎. 英語論文執筆のための統合ライティング支援環境. 第14回NLP若手の会 シンポジウム (YANS),
                            2019/08.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 鈴木潤, 乾健太郎. 文脈を考慮する言語モデルが捉える品詞情報とその軌跡. 第14回NLP若手の会 シンポジウム
                            (YANS), 2019/08.</li>
                        <li><u>栗林樹生</u>, 大内啓樹, 井之直也, Paul Reisert, 三好利昇, 鈴木潤, 乾健太郎. 
                            複数の言語単位に対するスパン表現を用いた論述構造解析. 言語処理学会第25回年次大会, pp.990-993, 2019/03.</li>
                        <li>*<u>栗林樹生</u>, *伊藤拓海, 内山香, 鈴木潤, 乾健太郎. (* 第一著者と第二著者の貢献度は等しい．)
                             言語モデルを用いた日本語の語順評価と基本語順の分析. 言語処理学会第25回年次大会, pp.1053-1056, 2019/03.
                        </li>
                        <li>*伊藤拓海, *<u>栗林樹生</u>, 小林隼人, 鈴木潤, 乾健太郎. (* 第一著者と第二著者の貢献度は等しい．)
                             ライティング支援を想定した情報補完型生成. 言語処理学会第25回年次大会, pp.970-973, 2019/03.
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Paul Reisert, Naoya Inoue and Kentaro Inui. Towards
                            Exploiting Argumentative Context for Argumentative Relation Identification. 
                            言語処理学会第24回年次大会, pp.284-287, 2018/03.</li>
                        <li><u>Tatsuki Kuribayashi</u>, Paul Reisert, Naoya Inoue, Kentaro Inui. Examining
                            Macro-level Argumentative Structure Features for Argumentative Relation Identification.
                             第4回自然言語処理シンポジウム・第234回自然言語処理研究会, 6pages, 2017/12.
                        </li>
                    </ul>
                    <h2>Teaching Assistant</h2>
                    <ul>
                        <li>Oct.2018-Mar.2019: Advanced Creative Engineering Training (Step-Qi school)</li>
                    </ul>
                    <h2>Skills</h2>
                    <ul>
                        <li>Python (6 years)</li>
                        <li>R (a little)</li>
                        <li>TypeScript, React (a litte...)</li>
                        <li>Tools in NLP/ML research: git, Docker, GCP, pytorch, w&b, TeX...</li>
                        <li>Designing API specifications for NLP models (working as a subcontractor)</li>
                        <li>Developing evaluation systems for NLP competition (working as a subcontractor)</li>
                    </ul>
                    <h2>Hobbies</h2>
                    <ul>
                        <li>Playing the saxophone (around 10 years)</li>
                        <li>Skiing</li>
                    </ul>
                </article>
            </div>
        </div>
        <footer>
        </footer>
    </div>
</body>

</html>
