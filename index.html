<!doctype html>
<html lang="">

<head>
    <meta charset="utf-8" />
    <title>Tatsuki Kuribayashi</title>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-92GGQSGY3S"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-92GGQSGY3S');
    </script>

    <meta name="author" content="Tatsuki Kuribayashi">
    <link rel="top" href="#" />
    <link
        href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600,300italic,400italic,600italic|Source+Code+Pro'
        rel='stylesheet' type='text/css'>
    </link>
    <link rel="stylesheet" href="kuribayashi4.github.io/theme/css/main.css" type="text/css" />
    <link href="kuribayashi4.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
        title="Tatsuki Kuribayashi Atom Feed" />

    <style>
    .typewriter span {
        font-family: monospace;
        color:#0000;
        background:
            linear-gradient(-90deg,#a8afb0 5px,#0000 0) 10px 0,
            linear-gradient(#2e302e 0 0) 0 0;
        background-size:calc(var(--n)*1ch) 200%;
        -webkit-background-clip:padding-box,text;
        background-clip:padding-box,text;
        background-repeat:no-repeat;
        animation: 
            b .5s infinite steps(1),   
            t calc(var(--n)*.01s) steps(var(--n)) forwards;
        }
        @keyframes t{
        from {background-size:0 200%}
        }
        @keyframes b{
        50% {background-position:0 -100%,0 0}
        }
        .highlight-news {
            background-color: #f0f0f0; /* è–„ã„ã‚°ãƒ¬ãƒ¼èƒŒæ™¯ */
            border-left: 5px solid #888; /* å·¦ã«å¼·èª¿ç·š */
            padding: 1em;
            margin: 1em 0;
            border-radius: 5px;
            font-size: 15px;
            font-family: 'Source Sans Pro', sans-serif;
        }
        .highlight-news p {
            margin: 0;
            font-weight: bold;
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="wrapper">
            <div role="main" class="content">
                <article style="padding: 2%">
                    <h1>Tatsuki Kuribayashi (æ —æ—æ¨¹ç”Ÿ)</h1>
                    <div style="width: 55%; min-width:480px; font-size: 10pt; font-family: 'Courier New', Courier, monospace;">
                        <span>My Japanese name (æ —æ—æ¨¹ç”Ÿ) is full of kanji related to <font style="color: #117511;">tree (æœ¨)</font> as my parents run a flower shop and love gardening, and as indicated literally, I've also been interested in trees (in language)!</span>
                        <br>
                        <br>
                        <span>My research is mainly focused on the interdisciplinary (NLP and cogsci) field of understanding incremental sentence processing, discourse processing, and reasoning process underlying in machines/humans.</span>
                    </div>
                    <table>
                        <tr style="align-items: left;">
                          <td style="width: 150px; display: inline-block; margin-top: 5%;">
                            <img src="./images/face_2024.png" alt="å†™çœŸ" title="profile" width="140" height="145">
                          </td>
                          <td style="width: 60%; display: inline-block">
                            <ul>
                              <li>Postdoc in <a href="https://mbzuai.ac.ae/research/department/natural-language-processing-department/">MBZUAI</a>. Previously, I was in <a href="https://www.nlp.ecei.tohoku.ac.jp/">Tohoku NLP Group</a>.</li>
                              <li>Email: Tatsuki.Kuribayashi at mbzuai.ac.ae</li>
                              <li><a href="https://scholar.google.co.jp/citations?user=-bqmkaAAAAAJ&hl=ja">Google Scholar</a></li>
                              <li><a href="https://twitter.com/ttk_kuribayashi?lang=en">Twitter</a></li>
                              <li><a href="https://github.com/kuribayashi4">Github</a></li>
                              <li><a href="https://docs.google.com/document/d/1EyIgvUHRHemEshNcW6z2uZyPTyh5bzS_UJl_HnN1U7Q/edit?usp=sharing">CV</a></li>
                            </ul>
                          </td>
                        </tr>
                      </table>
                    <h2>News</h2>
                    <div class="highlight-news" style="width: 70%;">
                    <p>In August 2025, I'll begin a tenure-track Assistant Professor position in the <a href="https://mbzuai.ac.ae/research-department/natural-language-processing-department/">NLP department at MBZUAI</a>.</p>
                    <br>
                    <p>ğŸ™Œ I'm looking for a postdoc and short-/mid-term visitors </p> <br>
                    I'm generally interested in positioning NLP in interdiciplinary, scientific context and also excited about collaborating with people from different background (linguistics, psychology, statistics...).
                    That said, cognitive modeling, LLM interpretability, etc. (see my recent publications) are areas where I can be more useful.<br><br>
                    MBZUAI is increasingly becoming a hub for broad CS fields (CV, Robotics, HCI...) with talented researchers, and I hope here you can explore your own unique areas, including linguistically-motivated computational experiments, for example, multimodal language acquisition simulation. <br><br>
                    If you're interested in any opportunity, contact me (tatsuki.kuribayashi [at] mbzuai.ac.ae) with your CV and a short description of your research interests. I'll also be able to (co-)supervise PhD students, and in this case, you can also directly apply the PhD course at MBZUAI <a href="https://mbzuai.ac.ae/study/admission-process/">here</a>.
                    </div>
                    <ul>
                    <li>2025/09: I'll give an invited talk at Evidence-based Linguistics Workshop 2025, NINJAL, Japan.</li>
                    <li>2025/05: CMCL 2025 has been held in NAACL 2025 [<a href="https://cmclorg.github.io/">web</a>]<br>
                    </li>
                    <li>2024/08: CMCL 2024 has been held in ACL 2024, Bangkok. [<a href="https://aclanthology.org/volumes/2024.cmcl-1/">proceedings</a>]
                    <li>2024/05: I gave a talk at ETH ZÃ¼rich [<a href="https://speakerdeck.com/kuribayashi4/from-cognitive-modeling-to-typological-universals-investigations-with-large-language-models">slides</a>], and at The Hong Kong Polytechnic University (online) [<a href="https://speakerdeck.com/kuribayashi4/cognitive-im-plausibility-of-large-language-models">slides</a>]</li>
                    <li>2023/04: I joined MBZUAI as a postdoctoral research fellow (advisor: <a href="https://eltimster.github.io/www/">Prof. Timothy Baldwin</a>).</li></li>
                    </ul>
                    <h2>Publications</h2>
                    <h3>Preprints</h3>
                    <ul>
                        <li>Tianyang Xu, <u>Tatsuki Kuribayashi</u>, Yohei Oseki, Ryan Cotterell, Alex Warstadt.<br>
                        "Can Language Models Learn Typologically Implausible Languages?"<br>
                        [<a href="https://arxiv.org/abs/2502.12317">arXiv</a>]
                        </li>
                        <li>
                            Riku Kisako, <u>Tatsuki Kuribayashi</u>, Ryohei Sasano.<br>
                            "On Representational Dissociation of Language and Arithmetic in Large Language Models."<br>
                            [<a href="https://arxiv.org/abs/2502.11932">arXiv</a>]
                        </li>
                        <li>
                            Keito Kudo, Yoichi Aoki, <u>Tatsuki Kuribayashi</u>, Shusaku Sone, Masaya Taniguchi, Ana Brassard, Keisuke Sakaguchi, Kentaro Inui.<br>
                            "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Step Reasoning."<br>
                            [<a href="https://arxiv.org/abs/2412.01113">arXiv</a>]
                        </li>
                    </ul>
                    <h3>Refereed papers</h3>
                    <ul>
                        <li><u>Tatsuki Kuribayashi</u>, Yohei Oseki, Souhaib Ben Taieb, Kentaro Inui, Timothy Baldwin.<br>
                            "Large Language Models Are Human-Like Internally."<br>
                            Transactions of the Association for Computational Linguistics (TACL).<br>
                            [<a href="https://arxiv.org/abs/2502.01615">arXiv</a> (will be updated soon) | <a href="https://github.com/kuribayashi4/surprisal_internal_layers">code</a>]
                        </li>
                        <li>Nadine El-Naggar, <u>Tatsuki Kuribayashi</u>, Ted Briscoe<br>
                            "GCG-Based Artificial Languages for Evaluating Inductive Biases of Neural Language Models."<br>
                            In Proceedings of Conference on Computational Natural Language Learning (CoNLL 2025), 2025/08.<br>
                            [<a href="https://openreview.net/pdf?id=Pju6ixAvq5">paper</a>]
                        </li>
                        <li>Rena Wei Gao, Xuetong Wu, <u>Tatsuki Kuribayashi</u>, Mingrui Ye, Siya Qi, Carsten Roever, Yuanxing Liu, Zheng Yuan, Jey Han Lau.<br>
                            "Can LLMs Simulate L2-English Dialogue? An Information-Theoretic Analysis of L1-Dependent Biases."<br>
                            In Proceedings of The 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025, main long), 2025/08.<br>
                        [<a href="https://arxiv.org/abs/2502.14507">arXiv</a>]</li>
                        <li>Mengyu Ye, <u>Tatsuki Kuribayashi</u>, Goro Kobayashi, Jun Suzuki. <br>
                            "Can Input Attributions Interpret the Inductive Reasoning Process in In-Context Learning?"<br>
                            Findings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025, Findings long), 2025/08.<br>
                            [<a href="https://arxiv.org/abs/2412.15628">arXiv</a>]
                        </li>
                        <li>Ryo Ueda, <u>Tatsuki Kuribayashi</u>, Shunsuke Kando, Kentaro Inui.<br>
                            "Syntactic Learnability of Echo State Neural Language Models at Scale."<br>
                            The 14th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2025, Non-archival), 2025/05.<br>
                            [<a href="https://arxiv.org/abs/2503.01724">arXiv</a>]
                        </li>
                        <li>Haonan Li, Xudong Han, ..., <u>Tatsuki Kuribayashi</u>, ..., Eduard Hovy, Iryna Gurevych, Preslav Nakov, Monojit Choudhury, Timothy Baldwin.<br>
                            "Libra-Leaderboard: Towards Responsible AI through a Balanced Leaderboard of Safety and Capability."<br>
                            In Proceedings of 2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL 2025, system demonstrations track), 2025/04.<br>
                            [<a href="https://arxiv.org/abs/2412.18551">arXiv</a>]
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Timothy Baldwin.<br>
                            "Does Vision Accelerate Hierarchical Generalization of Neural Language Learners?" <br>
                            In Proceedings of The 31st International Conference on Computational Linguistics (COLING 2025, long), 2025/01.<br>
                            [<a href="https://aclanthology.org/2025.coling-main.127/">paper</a> | <a href="https://arxiv.org/abs/2302.00667">arXiv</a>]</li>
                        <li>
                            David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo,...,<u>Tatsuki Kuribayashi</u>,...,Thamar Solorio, Alham Fikri Aji.<br>
                            "CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark."<br>
                            In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS 2024 Datasets and Benchmarks Track),  2024/12.</br>
                            [<a href="https://neurips.cc/virtual/2024/poster/97798">paper</a> | <a href="https://arxiv.org/abs/2406.05967">arXiv</a>]
                        </li>
                        <li>
                            Yoichi Aoki, Keito Kudo, <u>Tatsuki Kuribayashi</u>, Shusaku Sone, Masaya Taniguchi, Keisuke Sakaguchi, Kentaro Inui.<br>
                            "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning."<br>
                            In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024, main short), 2024/12.<br>
                            [<a href="https://aclanthology.org/2024.emnlp-main.789/">paper</a> | <a href="https://arxiv.org/abs/2406.16078">arXiv</a>]
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Ryo Ueda, Ryo Yoshida, Yohei Oseki, Ted Briscoe, Timothy Baldwin.<br>
                            "Emergent Word Order Universals from Cognitively-Motivated Language Models." <br>
                            In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024, main long), 2024/08. (acceptance rate: 940/4407=21.3%)<br>
                            [<a href="https://aclanthology.org/2024.acl-long.781/">paper</a> | <a href="https://arxiv.org/abs/2402.12363">arXiv</a>]</li>
                        <li><u>Tatsuki Kuribayashi</u>, Yohei Oseki, Timothy Baldwin.<br>
                            "Psychometric Predictive Power of Large Language Models." <br>
                            Findings of the 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2024, Findings long), 2024/06. (acceptance rate: top 869/2434=35.7%)<br>
                            [<a href="https://aclanthology.org/2024.findings-naacl.129/">paper</a> | <a href="https://arxiv.org/abs/2311.07484">arXiv</a>]</li>
                        <li>
                            å¤§ç¾½æœªæ‚ , <u>æ —æ—æ¨¹ç”Ÿ</u>, å¤§å†…å•“æ¨¹, æ¸¡è¾ºå¤ªéƒ. <br>
                             è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç¬¬äºŒè¨€èªç²å¾—. <br>
                             è‡ªç„¶è¨€èªå‡¦ç† (Japan domestic journal), Volume 31, Number 2, pp.433-455, 2024/06.<br>
                             [<a href="https://www.jstage.jst.go.jp/article/jnlp/31/2/31_433/_article/-char/ja">paper</a>]
                        </li>
                        <li>Yukiko Ishizuki, <u>Tatsuki Kuribayashi</u>, Yuichiroh Matsubayashi, Ryohei Sasano and Kentaro Inui.<br>
                            "To Drop or Not to Drop? Predicting Argument Ellipsis Judgments: A Case Study in Japanese."<br>
                            In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024, long), 2024/05. (acceptance rate: 1556/3417=52%)<br>
                            [<a href="https://aclanthology.org/2024.lrec-main.1408/">paper</a> | <a href="https://arxiv.org/abs/2404.11315">arXiv</a>]</li>
                        </li>
                        <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                            "Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps."<br>
                            In Proceedings of the 12th International Conference on Learning Representations (ICLR 2024, <b>spotlight, top 5%</b>), 2024/05. (acceptance rate: 2260/7262=31%)<br>
                            [<a href="https://openreview.net/forum?id=mYWsyTuiRp">paper</a> | <a href="https://arxiv.org/abs/2302.00456">arXiv</a>]</li>
                        <li>Mengyu Ye, <u>Tatsuki Kuribayashi</u>, Jun Suzuki, Hiroaki Funayama and Goro Kobayashi.<br>
                            "Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism."<br>
                            In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP-2023, main short), 2023/12. (acceptance rate: 146/1041=14.0%)<br>
                            [<a href="https://aclanthology.org/2023.emnlp-main.912/">paper</a> | <a href="https://arxiv.org/abs/2310.14868v1">arXiv</a>]
                        <ul>
                            <li>Mengyu Ye, <u>Tatsuki Kuribayashi</u>, Jun Suzuki, Hiroaki Funayama and Goro Kobayashi.<br>
                            "Assessing Chain-of-Thought Reasoning against Lexical Negation: A Case Study on Syllogism."<br>
                            In Proceedings of Student Research Workshop (SRW) at the 61st Annual Meeting of the Association for Computational Linguistics 2023 (ACL-SRW, Non-archival, <b>best paper award</b>), 2023/07.<br>
                        </ul>
                        </li>
                        <li>
                            Takumi Ito, Naomi Yamashita, <u>Tatsuki Kuribayashi</u>, Masatoshi Hidaka, Jun Suzuki, Ge Gao, Jack Jamieson and Kentaro Inui.<br>
                            "Use of an AI-powered Rewriting Support Software in Context with Other Tools: A Study of Non-Native English Speakers."<br>
                            The ACM Symposium on User Interface Software and Technology 2023 (UIST 2023), 2023/10.<br>
                            [<a href="https://dl.acm.org/doi/abs/10.1145/3586183.3606810">paper</a>]
                        </li>
                        <li>Miyu Oba, <u>Tatsuki Kuribayashi</u>, Hiroki Ouchi, Taro Watanabe. <br>
                            "Second Language Acquisition of Neural Language Models."<br>
                            Findings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL-2023, Findings long), 2023/07. (acceptance rate: top 39.1%)<br>
                            [<a href="https://aclanthology.org/2023.findings-acl.856/">paper</a> | <a href="https://arxiv.org/abs/2306.02920">arXiv</a>]
                        </li>
                        <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi and Kentaro Inui. <br>
                            "Transformer Language Models Handle Word Frequency in Prediction Head." <br>
                            Findings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL-2023, Findings short), 2023/07. (acceptance rate: top 39.1%)<br>
                            [<a href="https://aclanthology.org/2023.findings-acl.276/">paper</a> | <a href="https://arxiv.org/abs/2305.18294">arXiv</a>]
                        </li>
                        <li>Keito Kudo, Yoichi Aoki, <u>Tatsuki Kuribayashi</u>, Ana Brassard, Masashi Yoshikawa, Keisuke Sakaguchi and Kentaro Inui. <br>
                            "Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?" <br>
                            Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2023, main short), 2023/05. (acceptance rate: 281/1166=24.1%)<br>
                            [<a href="https://aclanthology.org/2023.eacl-main.98/">paper</a> | <a href="https://arxiv.org/abs/2302.07866">arXiv</a>]
                        </li>
                        <li>Yoichi Aoki, Keito Kudo, <u>Tatsuki Kuribayashi</u>, Ana Brassard, Masashi Yoshikawa, Keisuke Sakaguchi and Kentaro Inui. <br>
                            "Empirical Investigation of Neural Symbolic Reasoning Strategies." <br>
                            Findings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2023, Findings short), 2023/05. (acceptance rate: top 482/1166=41.3%)<br>
                            [<a href="https://aclanthology.org/2023.findings-eacl.86/">paper</a> | <a href="https://arxiv.org/abs/2302.08148">arXiv</a>]
                        </li>
                        <ul>
                            <li>Yoichi Aoki, Keito Kudo, <u>Tatsuki Kuribayashi</u>, Ana Brassard, Masashi Yoshikawa, Keisuke Sakaguchi and Kentaro Inui. <br>
                                "Empirical Investigation of Neural Symbolic Reasoning Strategies." <br>
                                Non-archival submission for the 2022 AACL-IJCNLP Student Research Workshop (AACL-IJCNLP SRW Non-archival, <b>best paper award</b>), 2022/11.</li>
                        </ul>
                        <li><u>Tatsuki Kuribayashi</u>, Yohei Oseki, Ana Brassard, Kentaro Inui.<br>
                            "Context Limitations Make Neural Language Models More Human-Like."<br>
                            In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP-2022, main long), pp.10421-10436, 2022/12. (acceptance rate: 829/4190=20%)<br>
                            [<a href="https://aclanthology.org/2022.emnlp-main.712/">paper</a> | <a href="https://arxiv.org/abs/2205.11463">arXiv</a>]
                        </li>
                        <li>
                            Riki Fujihara, <u>Tatsuki Kuribayashi</u>, Kaori Abe, Ryoko Tokuhisa, Kentaro Inui.<br>
                            "Topicalization in Language Models: A Case Study on Japanese."<br> 
                            In Proceedings of the 29th International Conference on Computational Linguistics (COLING-2022, long), pp.851-862, 2022/10 (acceptance rate: 522/1563=33.4%).<br>
                            [<a href="https://aclanthology.org/2022.coling-1.71/">paper</a>]
                        </li>
                        <ul>
                            <li>Riki Fujihara, <u>Tatsuki Kuribayashi</u>, Kaori Abe, Kentaro Inui.<br>
                                "Topicalization in Language Models: A Case Study on Japanese."<br>
                                In proceedings of Student Research Workshop at the Joint Conference of the 59th Annual
                                Meeting of the Association for Computational Linguistics and the 11th International Joint
                                Conference on Natural Language Processing (ACL-IJCNLP-2021 SRW Non-archival), 2021/08
                                (acceptance rate: 39%).<br>
                            </li>
                        </ul>
                        <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                            "Incorporating Residual and Normalization Layers into Analysis of Masked Language
                            Models."<br> In proceedings of the 2021 Conference on Empirical Methods in Natural Language
                            Processing (EMNLP 2021, main long), pp. 4547-4568, 2021/11 (acceptance rate: 23.3%).<br>
                            [<a href="https://aclanthology.org/2021.emnlp-main.373/">paper</a> | <a href="https://arxiv.org/abs/2109.07152">arXiv</a> | code]
                        </li>
                        <li>Hiroki Ouchi, Jun Suzuki, Sosuke Kobayashi, Sho Yokoi, <u>Tatsuki Kuribayashi</u>, Masashi Yoshikawa, Kentaro Inui.<br>
                            "Instance-Based Neural Dependency Parsing."<br> Transactions of the Association for Computational Linguistics 2021 (TACL 2021), 2021/09.<br>
                            [<a href="https://aclanthology.org/2021.tacl-1.89/">paper</a> | <a href="https://arxiv.org/abs/2109.13497">arXiv</a>]
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Yohei Oseki, Takumi Ito, Ryo Yoshida, Masayuki Asahara,
                            Kentaro Inui.<br>
                            "Lower Perplexity is Not Always Human-Like."<br>
                            In proceedings of the Joint Conference of the 59th Annual Meeting of the Association for
                            Computational Linguistics and the 11th International Joint Conference on Natural Language
                            Processing (ACL-IJCNLP 2021, main long), pp. 5203-5217, 2021/08 (acceptance rate: 21.3%).<br>
                            [<a href="https://aclanthology.org/2021.acl-long.405/">paper</a> | <a
                                href="https://github.com/kuribayashi4/surprisal_reading_time_en_ja">code</a>]
                        </li>
                        <li><u>æ —æ—æ¨¹ç”Ÿ</u>, å¤§å†…å•“æ¨¹, äº•ä¹‹ä¸Šç›´ä¹Ÿ, éˆ´æœ¨æ½¤, Paul Reisert, ä¸‰å¥½åˆ©æ˜‡, ä¹¾å¥å¤ªéƒ<br>
                            ã€Œè«–è¿°æ§‹é€ è§£æã«ãŠã‘ã‚‹ã‚¹ãƒ‘ãƒ³åˆ†æ•£è¡¨ç¾ã€<br>
                            è‡ªç„¶è¨€èªå‡¦ç† (domestic journal), Volume 27, Number 4, pp.753-780, 2020/12.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jnlp/27/4/27_753/_article/-char/ja/">paper</a> |
                            <a href="./pdfs/20210308_Argmining_kuribayashi_public.pdf">slides</a>]
                        </li>
                        <li>Takaki Otake, Sho Yokoi, Naoya Inoue, Ryo Takahashi, <u>Tatsuki Kuribayashi</u>, Kentaro
                            Inui.<br>
                            "Modeling Event Salience in a Narrative Based on Barthes' Cardinal Function."<br>
                            In proceedings of the 28th International Conference on Computational Linguistics
                            (COLING-2020, short), pp. 1784-1794, 2020/12 (acceptance rate: 26.2%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.coling-main.160/">paper</a> | <a
                                href="https://arxiv.org/abs/2011.01785">arXiv</a>]
                        </li>
                        <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                            "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms."<br>
                            In proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
                            (EMNLP-2020, main long), pp. 7057-7075, 2020/11 (acceptance rate: 754/3359=22.4%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.emnlp-main.574/">paper</a> | <a
                                href="https://arxiv.org/abs/2004.10102">arXiv</a> | <a
                                href="https://github.com/gorokoba560/norm-analysis-of-transformer">code</a>]
                        </li>
                        <ul>
                            <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                                "Self-Attention is Not Only a Weight: Analyzing BERT with Vector Norms."<br>
                                In proceedings of Student Research Workshop at the 58th Annual Meeting of the Association
                                for Computational Linguistics (ACL-SRW-2020 Non-archival), 2020/07 (acceptance rate:
                                72/202=35.6%).<br>
                            </li>
                        </ul>
                        <li>*Takumi Ito, *<u>Tatsuki Kuribayashi</u>, *Masatoshi Hidaka, Jun Suzuki, Kentaro Inui. (*
                            equal contribution)<br>
                            "Langsmith: An Interactive Academic Text Revision System."<br>
                            In proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
                            (EMNLP-2020, demo), pp. 216-226, 2020/11 (acceptance
                            rate: ???%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.emnlp-demos.28/">paper</a> | <a
                                href="https://arxiv.org/abs/2010.04332">arXiv</a> | <a
                                href="https://emnlp-demo.editor.langsmith.co.jp/">demo</a>]
                        </li>
                        <li>Takumi Ito, <u>Tatsuki Kuribayashi</u>, Hayato Kobayashi, Ana Brassard, Masato Hagiwara, Jun
                            Suzuki, Kentaro Inui.<br>
                            "Assisting Authors to Convert Raw Products into Polished Prose."<br>
                            Journal of Cognitive Science, Vol.21, No.1, pp.99-135, 2020.<br>
                            [paper]</li>
                        <li><u>Tatsuki Kuribayashi</u>, Takumi Ito, Jun Suzuki, Kentaro Inui.<br>
                            "Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in
                            Japanese."<br>
                            In proceedings of the 58th annual meeting of the Association for Computational Linguistics
                            (ACL-2020, main long), pp. 6452-6459, 2020/07 (acceptance rate: 779/3429=22.7%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.acl-main.47/">paper</a> | <a
                                href="./pdfs/revised_20211223.pdf">updated version</a> | <a
                                href="https://github.com/kuribayashi4/LM_as_Word_Order_Evaluator">data</a> | <a
                                href="./pdfs/20200606_ACL2020.pptx">presentation source</a>]
                        </li>
                        <li>Hiroki Ouchi, Jun Suzuki, Sosuke Kobayashi, Sho Yokoi, <u>Tatsuki Kuribayashi</u>, Ryuto
                            Konno, Kentaro Inui.<br>
                            "Instance-Based Learning of Span Representations: A Case Study through Named Entity
                            Recognition."<br>
                            In proceedings of the 58th annual meeting of the Association for Computational Linguistics
                            (ACL-2020, Short), pp. 6452-6459, 2020/07 (acceptance rate: 208/1185=17.6%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.acl-main.575/">paper</a> | <a
                                href="https://arxiv.org/abs/2004.14514">arXiv</a>]
                        </li>
                        <li>*Takumi Ito, *<u>Tatsuki Kuribayashi</u>, Hayato Kobayashi, Ana Brassard, Masato Hagiwara,
                            Jun Suzuki, Kentaro Inui. (* equal contribution)<br>
                            "Diamonds in the Rough: Generating Fluent Sentences from Early-stage Drafts for Academic
                            Writing Assistance." <br>
                            In Proceedings of the 12th International Conference on Natural Language Generation
                            (INLG-2019), pp. 40-53, 2019/10
                            (acceptance rate: 73/143=51.0%).<br>[<a
                                href="https://www.aclweb.org/anthology/W19-8606/">paper</a> | <a
                                href="https://arxiv.org/abs/1910.09180">arXiv</a> | <a
                                href="https://github.com/taku-ito/INLG2019_SentRev">data</a>]</li>

                        <li>Masato Hagiwara, Takumi Ito, <u>Tatsuki Kuribayashi</u>, Jun Suzuki, Kentaro Inui.<br>
                            "TEASPN: Framework and Protocol for Integrated Writing Assistance Environments." <br>
                            In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
                            (EMNLP-IJCNLP 2019, demo), pp. 229-234, 2019/11 
                            (acceptance rate: ???%).<br>[<a href="https://www.aclweb.org/anthology/D19-3039">paper</a>|
                            <a href="https://arxiv.org/abs/1909.02621">arXiv</a> | <a
                                href="https://github.com/teaspn/teaspn-sdk">code</a>]
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Hiroki Ouchi, Naoya Inoue, Paul Reisert, Toshinori Miyoshi, Jun
                            Suzuki, Kentaro Inui.<br>
                            "An Empirical Study of Span Representations in Argumentation Structure Parsing." <br>
                            In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics
                            (ACL-2019, short), pp. 4691-4698, 2019/07 
                            (acceptance rate: 213/1163=18.2%).<br>[<a
                                href="https://www.aclweb.org/anthology/P19-1464">paper</a> | <a
                                href="https://github.com/kuribayashi4/span_based_argumentation_parser">code</a>]</li>

                        <li>Paul Reisert, Naoya Inoue, <u>Tatsuki Kuribayashi</u>, Kentaro Inui.<br>
                            "Feasible Annotation Scheme for Capturing Policy Argument Reasoning using Argument
                            Templates." <br>
                            In Proceedings of the 5th Workshop on Argument Mining, pp.79-89, 2018/11 
                            (acceptance rate: 18/32=56.3%).<br>[<a
                                href="https://www.aclweb.org/anthology/W18-5210">paper</a> | <a
                                href="https://github.com/peldszus/arg-microtexts">data</a>]</li>
                    </ul>
                    <h2>Awards</h2>
                    <ul>
                        <li><a href="https://www.anlp.jp/nlp2024/award.html">2024/03: Outstanding Paper Award, NLP2024, Japan. è¨€èªå‡¦ç†å­¦ä¼šç¬¬30å›å¹´æ¬¡å¤§ä¼š, å„ªç§€è³</a>. ã©ã®ã‚ˆã†ãªè¨€èªãƒ¢ãƒ‡ãƒ«ãŒä¸å¯èƒ½ãªè¨€èªã‚’å­¦ç¿’ã—ã¦ã—ã¾ã†ã®ã‹ï¼Ÿâ€•èªé †æ™®éã‚’ä¾‹ã«â€•</li>
                        <li><a href="https://www.anlp.jp/nlp2024/award.html">2024/03: Special Committee Award, NLP2024, Japan. è¨€èªå‡¦ç†å­¦ä¼šç¬¬30å›å¹´æ¬¡å¤§ä¼š, å§”å“¡ç‰¹åˆ¥è³</a>. é•·è·é›¢ç›¸äº’ä½œç”¨ã™ã‚‹æ–‡è„ˆä¾å­˜è¨€èªã«ãŠã‘ã‚‹ç›¸è»¢ç§»ç¾è±¡ â€•è¨€èªãƒ¢ãƒ‡ãƒ«ã®å‰µç™ºç¾è±¡ã‚’çµ±è¨ˆåŠ›å­¦ã®è¦–ç‚¹ã§ç†è§£ã™ã‚‹â€•</li>
                        <li><a href="https://2023.aclweb.org/program/best_papers/">2023/07: ACL-SRW Best Paper Award</a>. Assessing Chain-of-Thought Reasoning against Lexical Negation: A Case Study on Syllogism.</li>
                        <li><a href="https://www.anlp.jp/nlp2023/award.html">2023/03: Special Committee Award, NLP2023, Japan. è¨€èªå‡¦ç†å­¦ä¼šç¬¬29å›å¹´æ¬¡å¤§ä¼š, å§”å“¡ç‰¹åˆ¥è³</a>. ç™¾èã¯ä¸€è¦‹ã«å¦‚ã‹ãšï¼Ÿè¦–è¦šæƒ…å ±ã¯è¨€èªãƒ¢ãƒ‡ãƒ«ã«æ–‡ã®éšå±¤æ§‹é€ ã‚’æ•™ç¤ºã™ã‚‹ã‹.</li>
                        <li><a href="https://www.anlp.jp/nlp2023/award.html">2023/03: Special Committee Award, NLP2023, Japan. è¨€èªå‡¦ç†å­¦ä¼šç¬¬29å›å¹´æ¬¡å¤§ä¼š, å§”å“¡ç‰¹åˆ¥è³</a>. æ—¥æœ¬èªè©±è€…ã®é …çœç•¥åˆ¤æ–­ã«é–¢ã™ã‚‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒ¢ãƒ‡ãƒªãƒ³ã‚°.</li>
                        <li><a href="https://nl-ipsj.or.jp/award/">2022/12: Excellent Research Award, ç¬¬254å› NLç ”, Japan.</a>. è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç¬¬äºŒè¨€èªç²å¾—åŠ¹ç‡.</li>
                        <li><a href="https://aacl2022-srw.github.io/program">2022/11: AACL-SRW Best Paper Award</a>. Empirical Investigation of Neural Symbolic Reasoning Strategies.</li>
                        <li><a href="https://yans.anlp.jp/entry/yans2022report">2022/08: Encouragement Awards (å¥¨åŠ±è³), YANS2022, Japan.</a> è¦–è¦šæƒ…å ±ã¯è¨€èªãƒ¢ãƒ‡ãƒ«ã«äººé–“ã‚‰ã—ã„çµ±èªçš„æ±åŒ–ã‚’ä¿ƒã™ã‹.</li>
                        <li>2022/03: President's Award, Graduate School of Information Sciences, Tohoku University. (æ±åŒ—å¤§å­¦ç·é•·è³)</li>
                        <li><a href="https://note.com/ipsj/n/nefc2ae945988">2021å¹´åº¦ç ”ç©¶ä¼šæ¨è–¦åšå£«è«–æ–‡</a>. Exploring Cognitive Plausibility of Neural NLP Models: Cross-Linguistic and Discourse-Level Studies.</li>
                        <li>2022/03: Excellent
                            Student in Electrical and Information Science Award, Tohoku University. (æ±åŒ—å¤§å­¦é›»æ°—ãƒ»æƒ…å ±ç³»å„ªç§€å­¦ç”Ÿè³)</li>
                        <li><a href="https://www.anlp.jp/nlp2022/award.html">2022/03: Special Committee Award, NLP2022, Japan. (è¨€èªå‡¦ç†å­¦ä¼šç¬¬28å›å¹´æ¬¡å¤§ä¼š, å§”å“¡ç‰¹åˆ¥è³) </a>Transformerã«ãŠã‘ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆã®ä½œç”¨</li>
                        <li><a href="https://www.anlp.jp/award/ronbun.html">2021/03: Best Paper Award 2020, Association for Natural Language
                                Processing (domestic journal). (è¨€èªå‡¦ç†å­¦ä¼š2020å¹´åº¦æœ€å„ªç§€è«–æ–‡è³) (1/28=3.5%)</a>
                            è«–è¿°æ§‹é€ è§£æã«ãŠã‘ã‚‹ã‚¹ãƒ‘ãƒ³åˆ†æ•£è¡¨ç¾<br>[<a
                                href="pdfs/20210308_Argmining_kuribayashi_public.pdf">å¹´æ¬¡å¤§ä¼šæ‹›å¾…è¬›æ¼”è³‡æ–™</a>]</li>
                        <li><a href="https://www.anlp.jp/nlp2021/">2021/03: Special Committee Award, NLP2021
                                (domestic conference). (è¨€èªå‡¦ç†å­¦ä¼šç¬¬27å›å¹´æ¬¡å¤§ä¼š, å§”å“¡ç‰¹åˆ¥è³) (14/361=3.8%)</a>
                            äºˆæ¸¬ã®æ­£ç¢ºãªè¨€èªãƒ¢ãƒ‡ãƒ«ãŒãƒ’ãƒˆã‚‰ã—ã„ã¨ã¯é™ã‚‰ãªã„</li>
                        <li><a href="https://www.nlp.ecei.tohoku.ac.jp/news-release/3571/">2020/03: Excellent
                                Student in Electrical and Information Science Award, Tohoku University. (æ±åŒ—å¤§å­¦é›»æ°—ãƒ»æƒ…å ±ç³»å„ªç§€å­¦ç”Ÿè³) </a></li>
                        <li><a href="https://www.anlp.jp/award/nenji.html">2020/03: Best Paper Award, NLP2020
                                (domestic conference) (è¨€èªå‡¦ç†å­¦ä¼šå¹´æ¬¡å¤§ä¼šæœ€å„ªç§€è³) (2/396=0.5%)</a> ãƒ™ã‚¯ãƒˆãƒ«â»‘ã«åŸºã¥ãè‡ªå·±æ³¨æ„æ©Ÿæ§‹ã®è§£æ. </li>
                        <li><a href="https://www.anlp.jp/award/nenji.html#y2019">2019/03: Young Researcher Award, 
                                NLP2019 (domestic conference). (è¨€èªå‡¦ç†å­¦ä¼šå¹´æ¬¡å¤§ä¼šè‹¥æ‰‹å¥¨åŠ±è³) (11/274=4.0%)</a>
                            è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®èªé †è©•ä¾¡ã¨åŸºæœ¬èªé †ã®åˆ†æ.</li>
                        <li><a href="http://www.ecei.tohoku.ac.jp/eipe/intro/news/commendation.html">2018/03: Excellent
                                Academic Award, School of Engineering, Tohoku University. (æ±åŒ—å¤§å­¦å·¥å­¦éƒ¨é•·è³) (26/910=2.9%)</a>
                        </li>
                    </ul>
                    <h2>Grant</h2>
                    <ul>
                        <li>2025/04-2028/03 (suspended from 2023/04 due to overseas affiliation): Grant-in-Aid for Early-Career Scientists, Japan. ç§‘å­¦ç ”ç©¶è²»åŠ©æˆäº‹æ¥­è‹¥æ‰‹ç ”ç©¶. ã€Œäººé–“ã‚‰ã—ã„è¨€èªç²å¾—åŠ¹ç‡ã«å¯¾ã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¨€èªå‡¦ç†ã‚’é€šã—ãŸæ§‹æˆè«–çš„æ¢æ±‚ã€</li>
                        <li>2020/04-2022/03: Doctoral Course (DC) Research Fellowships, Japan. æ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡(DC1) (é¢æ¥å…é™¤å†…å®š, 54/277=19.5%).
                            ã€Œãƒ†ã‚¯ã‚¹ãƒˆã®æ•°ç†çš„ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¨ã€æ•°ç†ãƒ¢ãƒ‡ãƒ«ã‚’é€šã—ãŸãƒ†ã‚¯ã‚¹ãƒˆã‚‰ã—ã•ã®è§£æ˜ã¸ã®æŒ‘æˆ¦ã€</li>
                    </ul>

                    <h2>Education/affiliation</h2>
                    <ul> 
                        <li>2025/02-Present: Visiting Researcher in Tohoku University.</li>
                        <li>2024/04-Present: Visiting Scientist in RIKEN.</li>
                        <li>2023/04-Present: Postdoctoral research fellow in <a href="https://mbzuai.ac.ae/research/department/natural-language-processing-department/">MBZUAI</a> (Advisor: Prof. <a href="https://people.eng.unimelb.edu.au/tbaldwin/">Timothy Baldwin</a>).
                        </li>
                        <li>2022/04-2023/03: Specially-appointed research fellow in <a href="https://www.nlp.ecei.tohoku.ac.jp/">Tohoku NLP Group</a>.
                        </li>
                        <li>2020/04-2022/03: PhD student of Information Science, Graduate School of Information
                            Sciences, Tohoku University, Japan. (Supervisor: <a
                                href="http://www.cl.ecei.tohoku.ac.jp/~inui/">Prof. Kentaro Inui</a>)<br>
                                [<a href="pdfs/2024_download_PhD_honshinsa_Thesis_Kuribayashi_.pdf">thesis</a>]</li>
                        <li>2018/04-2020/03: Master's student of Information Science, Graduate School of Information
                            Sciences, Tohoku University, Japan. (Supervisor: <a
                                href="http://www.cl.ecei.tohoku.ac.jp/~inui/">Prof. Kentaro Inui</a>) <br>
                            [<a href="http://www.cl.ecei.tohoku.ac.jp/local/handouts/2019/thesis/thesis-20200401-kuribayashi.pdf">thesis</a>]</li>
                        <li>2018/04-2022/03.: Graduate Program in Data Science, Tohoku University.</li>
                        <li>2014/04-2018/03: Bachelor of Engineering, Department of Information and Intelligent
                            Systems, Tohoku University, Japan. (Supervisor: <a
                                href="http://www.cl.ecei.tohoku.ac.jp/~inui/">Prof. Kentaro Inui</a>) </li>
                    </ul>
                    <h2>Activities/organizer</h2>
                    <ul>
                        <li>2024-Present: <a href="https://cmclorg.github.io/">CMCL Workshop</a> organizer</li>
                        <li>2018/05-2023/03: <a href="https://langsmith.co.jp/">Langsmith Co., Ltd.</a> (Co-founder).
                        </li>
                        <li>2020-2023: <a href="https://sites.google.com/view/snlp-jp">æœ€å…ˆç«¯NLPå‹‰å¼·ä¼š</a> (2023å®Ÿè¡Œå§”å“¡é•·)
                        </li>
                        <li>2021-2022: Member of NLP Dã®ä¼š, 2021-2022.</li>
                        <li>2019/08: <a href="https://www.uni-goettingen.de/en/data+science/594011.html">3rd Data
                            Science Summer School in GÃ¶ttingen</a>.</li>
                        <li>2017/08: <a href="https://internship.cookpad.com/2017/summer/17day-tech/">Cookpad 17days
                                Internship</a>.</li>
                    </ul>
                    <h2>Talks/writings</h2>
                    <ul>
                        <li>2025/03: Talk at MBZUAI [<a href="pdfs/Talk_slide.pdf">slides</a>]</li>
                        <li>2024/12: <u>æ —æ—æ¨¹ç”Ÿ</u>. å›æƒ³è¨˜äº‹ï¼šEmergent Word Order Universals from Cognitively-Motivated Language Models. [<a href="https://www.jstage.jst.go.jp/article/jnlp/31/4/31_1786/_article/-char/ja">link</a>]</li>
                        <li>2024/12: <u>æ —æ—æ¨¹ç”Ÿ</u>, å¤§é–¢æ´‹å¹³. é–‹å‚¬å ±å‘Šï¼šThe 13th Edition of the Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2024). [<a href="https://www.jstage.jst.go.jp/article/jnlp/31/4/31_1755/_article/-char/ja">link</a>]</li>
                        <li>2024/05: Talk at ETH ZÃ¼rich [<a href="https://speakerdeck.com/kuribayashi4/from-cognitive-modeling-to-typological-universals-investigations-with-large-language-models">slides</a>]</li>
                        <li>2024/05: Talk at The Hong Kong Polytechnic University (online) [<a href="https://speakerdeck.com/kuribayashi4/cognitive-im-plausibility-of-large-language-models">slides</a>]</li>
                        <li>2023/08: 
                            <a href="https://sites.google.com/view/snlp-jp/home/2022">æœ€å…ˆç«¯NLPå‹‰å¼·ä¼š</a>.
                            [<a
                                href="https://speakerdeck.com/kuribayashi4/zui-xian-duan-nlplun-wen-shao-jie-a-watermark-for-large-language-models">slides</a>]
                                                         [<a
                                href="https://speakerdeck.com/kuribayashi4/zui-xian-duan-nlplun-wen-shao-jie-why-does-surprisal-from-larger-transformer-based-language-models-provide-a-poorer-fit-to-human-reading-times">slides</a>]
                        </li>
                        <li>2023/02: è‡ªç„¶è¨€èªå‡¦ç†ã«ã‚ˆã‚‹è«–æ–‡åŸ·ç­†æ”¯æ´. æ±åŒ—å¤§å­¦ï¼ˆå¤§å­¦é™¢æ”¹é©æ¨é€²ã‚»ãƒ³ã‚¿ãƒ¼ä¸»å‚¬ï¼‰.
                            [<a href="https://speakerdeck.com/kuribayashi4/zi-ran-yan-yu-chu-li-niyorulun-wen-zhi-bi-zhi-yuan">slides</a>]
                        </li>
                        <li>
                            2022/12: Talk at MBZUAI. "Cognitive Plausibility of Neual Language Models."
                          [<a href="https://speakerdeck.com/kuribayashi4/cognitive-plausibility-of-neural-language-models">slides</a>]
                        </li>
                        <li>
                            2022/09: <a href="https://sites.google.com/view/snlp-jp/home/2022">æœ€å…ˆç«¯NLPå‹‰å¼·ä¼š</a>.
                            [<a
                                href="https://speakerdeck.com/kuribayashi4/zui-xian-duan-nlplun-wen-shao-jie-revisiting-the-uniform-information-density-hypothesis-emnlp2021-linguistic-dependencies-and-statistical-dependence-emnlp2021">slides</a>]
                        </li>
                        <li>
                            2021/09: <a href="https://sites.google.com/view/snlp-jp/home/2021">æœ€å…ˆç«¯NLPå‹‰å¼·ä¼š</a>.
                            [<a
                                href="./pdfs/snlp2021.pdf">slides</a>]
                        </li>
                        <li>2021/06: <u>æ —æ—æ¨¹ç”Ÿ</u>. Lower Perplexity is Not Always Human-Like (Talk). NLPã‚³ãƒ­ã‚­ã‚¦ãƒ . [<a
                            href="https://youtu.be/Xd_KfgWVWsI">å‹•ç”»</a>]</li>
                        <li>2021/06: <u>æ —æ—æ¨¹ç”Ÿ</u>. å­¦ä¼šè¨˜äº‹ã€Œè«–è¿°æ§‹é€ è§£æã«ãŠã‘ã‚‹ã‚¹ãƒ‘ãƒ³åˆ†æ•£è¡¨ç¾ã€ã®ç ”ç©¶ã‚’é€šã—ã¦. è‡ªç„¶è¨€èªå‡¦ç† (å­¦ä¼šè¨˜äº‹No.27), Volume 28, Number 2,
                                pp.677-681.</li>
                        <li>2020/09: <u>æ —æ—æ¨¹ç”Ÿ</u>. å­¦ä¼šè¨˜äº‹ Language Models as an Alternative Evaluator of Word Order Hypotheses: A
                                    Case Study in Japanese. è‡ªç„¶è¨€èªå‡¦ç† (å­¦ä¼šè¨˜äº‹), Volume 27, Number 3, pp.671-676.
                         </li>
                        <li>
                            2020/09: <a href="https://sites.google.com/view/snlp-jp/home/2020">æœ€å…ˆç«¯NLPå‹‰å¼·ä¼š</a>.
                            [<a
                                href="./pdfs/snlp2020.pdf">slides</a>]
                        </li>
                    </ul>
                    <h2>Tools</h2>
                    <ul>
                        <li>Langsmith Editor<br>[<a href="https://editor.langsmith.co.jp/">system</a> | <a
                                href="https://help.editor.langsmith.co.jp/">usage</a>] </li>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/xgzxLpcydG8" frameborder="0"
                            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen></iframe>
                        <li>TEASPN (Text Editing Assistance Smartness Protocol for Natural Language)<br>[<a
                                href="https://github.com/teaspn/teaspn-sdk">code</a> | <a
                                href="https://www.teaspn.org/">document</a>] </li>
                      <iframe width="560" height="315" src="https://www.youtube.com/embed/9DNqpudDehM" frameborder="0"
                            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen></iframe>
                    </ul>
                    <h2>Reviewer</h2>
                    <ul>
                        <li>Top conferences in NLP: 2021-present
                            <ul>
                            <li>ACL Rolling Review (typically on Interpretability and Analysis of Models for NLP / Linguistic theories, Cognitive Modeling, and Psycholinguistics)
                                <ul><li>2024/02-Present: Action Editor</li>
                                <li>2022/01-2023/12: Reviewer
                                    <li> Mentioned as a great reviewer: https://aclrollingreview.org/great-oct23-reviewers/ </li>
                                </li>
                            </ul>
                            </li>
                            <li>ACL: 2019 (secondary), 2020 (secondary), 2021 (Sentiment Analysis, Stylistic Analysis, and Argument Mining), 2023 (Linguistic Theories, Cognitive Modeling and Psycholinguistics.)</li>
                            <li>EMNLP: 2021 (Linguistic Theories, Cognitive Modeling and Psycholinguistics. Sentiment
                                Analysis, Stylistic Analysis, and Argument Mining), 2022 (Linguistic Theories, Cognitive Modeling and Psycholinguistics).</li>
                            </ul>
                        </li>
                        <li>COLM: 2024</li>
                        <li>COLING: 2020, 2022, 2024, 2025 (Language Modeling. Integrated Systems and Applications).</li>
                        <li>LREC: 2022, 2024 (Language Resources and Evaluation for Psycholinguistics, Cognitive Linguistics and Linguistic Theories. Natural Language Generation (including Summarization)).</li>
                        <li>INLG: 2020, 2021.</li>
                        <li>CoNLL: 2024, 2025.</li>
                        <li>BlackboxNLP: 2024.</li>
                        <li>2020-Present: è¨€èªå‡¦ç†å­¦ä¼šå¹´æ¬¡å¤§ä¼š (Japan domestic conference), å­¦ä¼šèªŒè‡ªç„¶è¨€èªå‡¦ç†.</li>
                    </ul>
                    <h2>(Japan) domestic conferences/articles</h3>
                    <ul>
                        <li><u>æ —æ— æ¨¹ç”Ÿ</u>, å¤§é–¢ æ´‹å¹³, Souhaib Ben Taieb, ä¹¾ å¥å¤ªéƒ, Timothy Baldwin. å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®æµ…ã„å±¤ãŒäººé–“ã®é€Ÿã„è¨€èªå‡¦ç†ã‚’å†ç¾ã™ã‚‹. è¨€èªå‡¦ç†å­¦ä¼šç¬¬31å›å¹´æ¬¡å¤§ä¼š, 4pages, 2025/3.</li>
                        <li>ä¸Šç”° äº®, <u>æ —æ— æ¨¹ç”Ÿ</u>, ç¥è—¤ é§¿ä»‹, ä¹¾ å¥å¤ªéƒ. RNNã®å›å¸°è¡Œåˆ—ã‚’å‡çµã—ã¦ã‚‚çµ±èªæ§‹é€ ã®ç²å¾—ã¯æãªã‚ã‚Œãªã„. è¨€èªå‡¦ç†å­¦ä¼šç¬¬31å›å¹´æ¬¡å¤§ä¼š, 4pages, 2025/3.</li>
                        <li>æœ¨è¿« ç’ƒç–, <u>æ —æ— æ¨¹ç”Ÿ</u>, ç¬¹é‡ é¼å¹³. å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å†…éƒ¨ã«ãŠã‘ã‚‹è¨€èªã¨è¨ˆç®—é ˜åŸŸã®åŒºåˆ†. è¨€èªå‡¦ç†å­¦ä¼šç¬¬31å›å¹´æ¬¡å¤§ä¼š, 4pages, 2025/3.</li>
                        <li><u>æ —æ—æ¨¹ç”Ÿ</u>, å¤§é–¢æ´‹å¹³, Timothy Baldwin. å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ–‡å‡¦ç†ã¯äººé–“ã‚‰ã—ã„ã®ã‹ï¼Ÿ. è¨€èªå‡¦ç†å­¦ä¼šç¬¬30å›å¹´æ¬¡å¤§ä¼š,4pages,2024/3.</li>
                        <li><u>æ —æ—æ¨¹ç”Ÿ</u>, ä¸Šç”°äº®, å‰ç”°é¼, å¤§é–¢æ´‹å¹³, Ted Briscoe, Timothy Baldwin. ã©ã®ã‚ˆã†ãªè¨€èªãƒ¢ãƒ‡ãƒ«ãŒä¸å¯èƒ½ãªè¨€èªã‚’å­¦ç¿’ã—ã¦ã—ã¾ã†ã®ã‹ï¼Ÿ---èªé †æ™®éã‚’ä¾‹ã«---. è¨€èªå‡¦ç†å­¦ä¼šç¬¬30å›å¹´æ¬¡å¤§ä¼š, 4pages, 2024/3.</li>
                        <li>é’æœ¨æ´‹ä¸€, å·¥è—¤æ…§éŸ³, æ›¾æ ¹å‘¨ä½œ, <u>æ —æ—æ¨¹ç”Ÿ</u>, è°·å£é›…å¼¥, å‚å£æ…¶ç¥, ä¹¾å¥å¤ªéƒ. è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ€è€ƒé€£é–çš„æ¨è«–ã«ãŠã‘ã‚‹æ¢ç´¢æˆ¦ç•¥ã®å‹•çš„å¤‰åŒ–. è¨€èªå‡¦ç†å­¦ä¼šç¬¬30å›å¹´æ¬¡å¤§ä¼š, 4pages, 2024/3.</li>
                        <li>å·¥è—¤æ…§éŸ³, é’æœ¨æ´‹ä¸€, <u>æ —æ—æ¨¹ç”Ÿ</u>, è°·å£é›…å¼¥, æ›¾æ ¹å‘¨ä½œ, å‚å£æ…¶ç¥, ä¹¾å¥å¤ªéƒ. ç®—è¡“æ¨è«–å•é¡Œã«ãŠã‘ã‚‹è‡ªå·±å›å¸°å‹è¨€èªãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨æ©Ÿåº. è¨€èªå‡¦ç†å­¦ä¼šç¬¬30å›å¹´æ¬¡å¤§ä¼š, 4pages, 2024/3.</li>
                        <li>è‘‰å¤¢å®‡, <u>æ —æ—æ¨¹ç”Ÿ</u>, å°æ—æ‚Ÿéƒ, éˆ´æœ¨æ½¤. æ–‡è„ˆå†…å­¦ç¿’ã«ãŠã‘ã‚‹æ–‡è„ˆå†…äº‹ä¾‹ã®å¯„ä¸åº¦æ¨å®š. è¨€èªå‡¦ç†å­¦ä¼šç¬¬30å›å¹´æ¬¡å¤§ä¼š, 4pages, 2024/3.</li>
                        <li>éƒ½åœ°æ‚ é¦¬, é«˜æ©‹æƒ‡, æ¨ªäº•ç¥¥, <u>æ —æ—æ¨¹ç”Ÿ</u>, ä¸Šç”°äº®, å®®åŸè‹±ä¹‹. é•·è·é›¢ç›¸äº’ä½œç”¨ã™ã‚‹æ–‡è„ˆä¾å­˜è¨€èªã«ãŠã‘ã‚‹ç›¸è»¢ç§»ç¾è±¡ -è¨€èªãƒ¢ãƒ‡ãƒ«ã®å‰µç™ºç¾è±¡ã‚’çµ±è¨ˆåŠ›å­¦ã®è¦–ç‚¹ã§ç†è§£ã™ã‚‹-. è¨€èªå‡¦ç†å­¦ä¼šç¬¬30å›å¹´æ¬¡å¤§ä¼š,4pages,2024/3.</li>
                        <li><u>æ —æ—æ¨¹ç”Ÿ</u>. ç™¾èã¯ä¸€è¦‹ã«å¦‚ã‹ãšï¼Ÿè¦–è¦šæƒ…å ±ã¯è¨€èªãƒ¢ãƒ‡ãƒ«ã«æ–‡ã®éšå±¤æ§‹é€ ã‚’æ•™ç¤ºã™ã‚‹ã‹. è¨€èªå‡¦ç†å­¦ä¼šç¬¬29å›å¹´æ¬¡å¤§ä¼š, 4pages, 2023/3.</li>
                        <li>å°æ—æ‚Ÿéƒ, <u>æ —æ—æ¨¹ç”Ÿ</u>, æ¨ªäº•ç¥¥, ä¹¾å¥å¤ªéƒ. Transformerè¨€èªãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãƒ˜ãƒƒãƒ‰å†…ãƒã‚¤ã‚¢ã‚¹ã«ã‚ˆã‚‹é »åº¦è£œæ­£åŠ¹æœ. è¨€èªå‡¦ç†å­¦ä¼šç¬¬29å›å¹´æ¬¡å¤§ä¼š, 4pages, 2023/3.</li>
                        <li>è‘‰å¤¢å®‡, <u>æ —æ—æ¨¹ç”Ÿ</u>, èˆŸå±±å¼˜æ™ƒ, éˆ´æœ¨æ½¤. æ€è€ƒé€£é–æŒ‡ç¤ºã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å¦å®šè¡¨ç¾ç†è§£. è¨€èªå‡¦ç†å­¦ä¼šç¬¬29å›å¹´æ¬¡å¤§ä¼š, 4pages, 2023/3.</li>
                        <li>è—¤åŸåç”Ÿ, <u>æ —æ—æ¨¹ç”Ÿ</u>, å¾³ä¹…è‰¯å­, ä¹¾å¥å¤ªéƒ. ä¸»é¡ŒåŒ–ã«ãŠã‘ã‚‹äººé–“ã¨è¨€èªãƒ¢ãƒ‡ãƒ«ã®å¯¾ç…§. è¨€èªå‡¦ç†å­¦ä¼šç¬¬29å›å¹´æ¬¡å¤§ä¼š, 4pages, 2023/3.</li>
                        <li>çŸ³æœˆç”±ç´€å­, <u>æ —æ—æ¨¹ç”Ÿ</u>, æ¾æ—å„ªä¸€éƒ, ç¬¹é‡é¼å¹³, ä¹¾å¥å¤ªéƒ. æ—¥æœ¬èªè©±è€…ã®é …çœç•¥åˆ¤æ–­ã«é–¢ã™ã‚‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒ¢ãƒ‡ãƒªãƒ³ã‚°. è¨€èªå‡¦ç†å­¦ä¼šç¬¬29å›å¹´æ¬¡å¤§ä¼š, 4pages, 2023/3.</li>
                        <li>å¤§ç¾½æœªæ‚ , <u>æ —æ—æ¨¹ç”Ÿ</u>, å¤§å†…å•“æ¨¹, æ¸¡è¾ºå¤ªéƒ. è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç¬¬äºŒè¨€èªç²å¾—. è¨€èªå‡¦ç†å­¦ä¼šç¬¬29å›å¹´æ¬¡å¤§ä¼š, 4pages, 2023/3.</li>
                        <li>å·¥è—¤æ…§éŸ³, é’æœ¨æ´‹ä¸€, <u>æ —æ—æ¨¹ç”Ÿ</u>, Ana Brassard, å‰å·å°†å¸, å‚å£æ…¶ç¥, ä¹¾å¥å¤ªéƒ. ç®—è¡“å•é¡Œã«ãŠã‘ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æ§‹æˆçš„æ¨è«–èƒ½åŠ›. è¨€èªå‡¦ç†å­¦ä¼šç¬¬29å›å¹´æ¬¡å¤§ä¼š, 4pages, 2023/3.</li>
                        <li>é’æœ¨æ´‹ä¸€, å·¥è—¤æ…§éŸ³, Ana Brassard, <u>æ —æ—æ¨¹ç”Ÿ</u>, å‰å·å°†å¸, å‚å£æ…¶ç¥, ä¹¾å¥å¤ªéƒ. ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«è¨˜å·æ¨è«–ã«ãŠã‘ã‚‹æ¨è«–éç¨‹ã®æ•™ç¤ºæ–¹æ³•. è¨€èªå‡¦ç†å­¦ä¼šç¬¬29å›å¹´æ¬¡å¤§ä¼š, 4pages, 2023/3.</li>
                        <li>å¤§ç¾½æœªæ‚ , <u>æ —æ—æ¨¹ç”Ÿ</u>, å¤§å†…å•“æ¨¹, æ¸¡è¾ºå¤ªéƒ. è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç¬¬äºŒè¨€èªç²å¾—åŠ¹ç‡. ç¬¬254å›è‡ªç„¶è¨€èªå‡¦ç†ç ”ç©¶ä¼š, 6pages, 2022/11.</li>
                        <li>é’æœ¨æ´‹ä¸€, å·¥è—¤æ…§éŸ³, <u>æ —æ—æ¨¹ç”Ÿ</u>, Ana Brassard, å‰å·å°†å¸, ä¹¾å¥å¤ªéƒ. æ¨è«–éç¨‹ã®æ€§è³ªãŒãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å¤šæ®µæ¨è«–èƒ½åŠ›ã«ä¸ãˆã‚‹å½±éŸ¿. ç¬¬17å›NLPè‹¥æ‰‹ã®ä¼š ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ  (YANS), 2022/08.</li>
                        <li><u>æ —æ—æ¨¹ç”Ÿ</u>. è¦–è¦šæƒ…å ±ã¯è¨€èªãƒ¢ãƒ‡ãƒ«ã«äººé–“ã‚‰ã—ã„çµ±èªçš„æ±åŒ–ã‚’ä¿ƒã™ã‹. ç¬¬17å›NLPè‹¥æ‰‹ã®ä¼š ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ  (YANS), 2022/08.</li>
                        <li>å¤§ç¾½æœªæ‚ , <u>æ —æ—æ¨¹ç”Ÿ</u>, å¤§å†…å•“æ¨¹, æ¸¡è¾ºå¤ªéƒ. è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç¬¬äºŒè¨€èªç²å¾—åŠ¹ç‡. ç¬¬17å›NLPè‹¥æ‰‹ã®ä¼š ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ  (YANS), 2022/08.</li>
                        <li>å°æ—æ‚Ÿéƒ, <u>æ —æ—æ¨¹ç”Ÿ</u>, æ¨ªäº•ç¥¥, ä¹¾å¥å¤ªéƒ. Transformerã«ãŠã‘ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆã®æ··ãœåˆã‚ã›ä½œç”¨. ç¬¬17å›NLPè‹¥æ‰‹ã®ä¼š ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ  (YANS), 2022/08.</li>
                        <li><u>æ —æ—æ¨¹ç”Ÿ</u>, å¤§é–¢æ´‹å¹³, Ana Brassard, ä¹¾å¥å¤ªéƒ. ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ã®éå‰°ãªä½œæ¥­è¨˜æ†¶. è¨€èªå‡¦ç†å­¦ä¼šç¬¬28å›å¹´æ¬¡å¤§ä¼š, pp.1530-1535, 2022/03.</li>
                        <li>å°æ—æ‚Ÿéƒ, <u>æ —æ—æ¨¹ç”Ÿ</u>, æ¨ªäº•ç¥¥, ä¹¾å¥å¤ªéƒ. Transformerã«ãŠã‘ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆã®ä½œç”¨. è¨€èªå‡¦ç†å­¦ä¼šç¬¬28å›å¹´æ¬¡å¤§ä¼š, pp.1072-1077, 2022/03.</li>
                        <li>çŸ³æœˆç”±ç´€å­, <u>æ —æ—æ¨¹ç”Ÿ</u>, æ¾æ—å„ªä¸€éƒ, å¤§é–¢æ´‹å¹³. æƒ…å ±é‡ã«åŸºã¥ãæ—¥æœ¬èªé …çœç•¥ã®åˆ†æ. è¨€èªå‡¦ç†å­¦ä¼šç¬¬28å›å¹´æ¬¡å¤§ä¼š, pp.442-447, 2022/03.</li>
                        <li>é’æœ¨æ´‹ä¸€, å·¥è—¤æ…§éŸ³, Ana Brassard, <u>æ —æ—æ¨¹ç”Ÿ</u>, å‰å·å°†å¸, ä¹¾å¥å¤ªéƒ. å¤šæ®µã®æ•°é‡æ¨è«–ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹é©å¿œçš„ãªãƒ¢ãƒ‡ãƒ«ã®æŒ¯ã‚‹èˆã„ã®æ¤œè¨¼. è¨€èªå‡¦ç†å­¦ä¼šç¬¬28å›å¹´æ¬¡å¤§ä¼š, pp.168-172, 2022/03.</li>
                        <li>çŸ³æœˆç”±ç´€å­, <u>æ —æ—æ¨¹ç”Ÿ</u>, æ¾æ—å„ªä¸€éƒ, å¤§é–¢æ´‹å¹³ æƒ…å ±é‡ã«åŸºã¥ãæ—¥æœ¬èªé …çœç•¥ã®åˆ†æ. NLPè‹¥æ‰‹ã®ä¼š (YANS) ç¬¬16å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ,
                            2021/08.</li>
                        <li>å°æ—æ‚Ÿéƒ, <u>æ —æ—æ¨¹ç”Ÿ</u>, æ¨ªäº•ç¥¥, ä¹¾å¥å¤ªéƒ. éç·šå½¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åŠ æ³•åˆ†è§£ã«åŸºã¥ãTransformerãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è§£æ. NLPè‹¥æ‰‹ã®ä¼š (YANS)
                            ç¬¬16å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ , 2021/08.</li>
                        <li><u>æ —æ—æ¨¹ç”Ÿ</u>, å¤§é–¢æ´‹å¹³, ä¼Šè—¤æ‹“æµ·, å‰ç”°é¼, æµ…åŸæ­£å¹¸, ä¹¾å¥å¤ªéƒ. äºˆæ¸¬ã®æ­£ç¢ºãªè¨€èªãƒ¢ãƒ‡ãƒ«ãŒãƒ’ãƒˆã‚‰ã—ã„ã¨ã¯é™ã‚‰ãªã„. è¨€èªå‡¦ç†å­¦ä¼šç¬¬27å›å¹´æ¬¡å¤§ä¼š,
                            pp.267-272,
                            2021/03.</li>
                        <li><u>æ —æ—æ¨¹ç”Ÿ</u>, å¤§é–¢æ´‹å¹³, ä¼Šè—¤æ‹“æµ·, å‰ç”°é¼, æµ…åŸæ­£å¹¸, ä¹¾å¥å¤ªéƒ. æ—¥æœ¬èªã®èª­ã¿ã‚„ã™ã•ã«å¯¾ã™ã‚‹æƒ…å ±é‡ã«åŸºã¥ã„ãŸçµ±ä¸€çš„ãªè§£é‡ˆ. è¨€èªå‡¦ç†å­¦ä¼šç¬¬27å›å¹´æ¬¡å¤§ä¼š,
                            pp.723-728, 2021/03.</li>
                        <li>ä¼Šè—¤æ‹“æµ·, <u>æ —æ—æ¨¹ç”Ÿ</u>, æ—¥é«˜é›…ä¿Š, éˆ´æœ¨æ½¤, ä¹¾å¥å¤ªéƒ. Langsmith: äººã¨ã‚·ã‚¹ãƒ†ãƒ ã®å”åƒã«ã‚ˆã‚‹è«–æ–‡åŸ·ç­†. è¨€èªå‡¦ç†å­¦ä¼šç¬¬27å›å¹´æ¬¡å¤§ä¼š,
                            pp.1834-1839,
                            2021/03.</li>
                        <li>è—¤åŸåç”Ÿ, <u>æ —æ—æ¨¹ç”Ÿ</u>, ä¹¾å¥å¤ªéƒ. äººã¨è¨€èªãƒ¢ãƒ‡ãƒ«ãŒæ‰ãˆã‚‹æ–‡ã®ä¸»é¡Œ. è¨€èªå‡¦ç†å­¦ä¼šç¬¬27å›å¹´æ¬¡å¤§ä¼š, pp.1307-1312, 2021/03.</li>
                        <li>å°æ—æ‚Ÿéƒ, <u>æ —æ—æ¨¹ç”Ÿ</u>, æ¨ªäº•ç¥¥, ä¹¾å¥å¤ªéƒ. Transformerã®æ–‡è„ˆã‚’æ··ãœã‚‹ä½œç”¨ã¨æ··ãœãªã„ä½œç”¨. è¨€èªå‡¦ç†å­¦ä¼šç¬¬27å›å¹´æ¬¡å¤§ä¼š,
                            pp.1224-1229,
                            2021/03.</li>
                        <li>å¤§å†…å•“æ¨¹, éˆ´æœ¨æ½¤, å°æ—é¢¯ä»‹, æ¨ªäº•ç¥¥, <u>æ —æ—æ¨¹ç”Ÿ</u>, å‰å·å°†å¸,
                            ä¹¾å¥å¤ªéƒ. äº‹ä¾‹ãƒ™ãƒ¼ã‚¹ä¾å­˜æ§‹é€ è§£æã®ãŸã‚ã®ä¾å­˜é–¢ä¿‚è¡¨ç¾å­¦ç¿’. è¨€èªå‡¦ç†å­¦ä¼šç¬¬27å›å¹´æ¬¡å¤§ä¼š, pp.497-502, 2021/03.</li>
                        <li>å¤§ç«¹å­æ¨¹, æ¨ªäº•ç¥¥, äº•ä¹‹ä¸Šç›´ä¹Ÿ, é«˜æ©‹è«’, <u>æ —æ—æ¨¹ç”Ÿ</u>,
                            ä¹¾å¥å¤ªéƒ. ç‰©èªã«ãŠã‘ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã®é¡•ç¾æ€§æ¨å®šã¨ç‰©èªé¡ä¼¼æ€§è¨ˆç®—ã¸ã®å¿œç”¨. è¨€èªå‡¦ç†å­¦ä¼šç¬¬27å›å¹´æ¬¡å¤§ä¼š, pp.1324-1329, 2021/03.</li>
                        <li>è—¤åŸåç”Ÿ, <u>æ —æ—æ¨¹ç”Ÿ</u>, ä¹¾å¥å¤ªéƒ. æ—¥æœ¬èªè¨€èªãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã™ã‚‹æ–‡å½¢å¼ã®å‚¾å‘ã¨æ–‡è„ˆã®å½±éŸ¿ ãƒ¼ ä¸»é¡ŒåŒ–ãƒ»æœ‰æ¨™èªé †ã«ã¤ã„ã¦. NLPè‹¥æ‰‹ã®ä¼š (YANS)
                            ç¬¬15å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ , 2020/09.</li>
                        <li>å°æ—æ‚Ÿéƒ, <u>æ —æ—æ¨¹ç”Ÿ</u>, æ¨ªäº•ç¥¥, ä¹¾å¥å¤ªéƒ. ãƒ™ã‚¯ãƒˆãƒ«é•·ã«åŸºã¥ãæ³¨æ„æ©Ÿæ§‹ã¨æ®‹å·®çµåˆã®åŒ…æ‹¬çš„ãªåˆ†æ. NLPè‹¥æ‰‹ã®ä¼š (YANS) ç¬¬15å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ,
                            2020/09.</li>
                        <li><u>æ —æ—æ¨¹ç”Ÿ</u>, ä¼Šè—¤æ‹“æµ·, éˆ´æœ¨æ½¤, ä¹¾å¥å¤ªéƒ. æ—¥æœ¬èªèªé †åˆ†æã«è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã‚‹ã“ã¨ã®å¦¥å½“æ€§ã«ã¤ã„ã¦. è¨€èªå‡¦ç†å­¦ä¼šç¬¬26å›å¹´æ¬¡å¤§ä¼š,
                            pp.493-496,
                            2020/03.<br>[<a href="./pdfs/nlp2020.pdf">slides</a>]</li>
                        <li>å°æ— æ‚Ÿéƒ, <u>æ —æ—æ¨¹ç”Ÿ</u>, æ¨ªäº• ç¥¥, éˆ´æœ¨æ½¤, ä¹¾å¥å¤ªéƒ. ãƒ™ã‚¯ãƒˆãƒ«â»‘ã«åŸºã¥ãè‡ªå·±æ³¨æ„æ©Ÿæ§‹ã®è§£æ. è¨€èªå‡¦ç†å­¦ä¼šç¬¬26å›å¹´æ¬¡å¤§ä¼š, pp.965-968,
                            2020/03.</li>
                        <li>å¤§å†… å•“æ¨¹, éˆ´æœ¨ æ½¤, å°æ— é¢¯ä»‹, æ¨ªäº• ç¥¥, <u>æ —æ—æ¨¹ç”Ÿ</u>, ä¹¾å¥å¤ªéƒ. ã‚¹ãƒ‘ãƒ³é–“ã®é¡ä¼¼æ€§ã«åŸºã¥ãäº‹ä¾‹ãƒ™ãƒ¼ã‚¹æ§‹é€ äºˆæ¸¬. è¨€èªå‡¦ç†å­¦ä¼šç¬¬26å›å¹´æ¬¡å¤§ä¼š,
                            pp.331-334, 2020/03.</li>
                        <li>å¤§ç«¹ å­æ¨¹, æ¨ªäº• ç¥¥, äº•ä¹‹ä¸Š ç›´ä¹Ÿ, é«˜æ©‹ è«’, <u>æ —æ—æ¨¹ç”Ÿ</u>, ä¹¾å¥å¤ªéƒ. è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç‰©èªä¸­ã®ã‚¤ãƒ™ãƒ³ãƒˆã®é¡•ç¾æ€§æ¨å®š. è¨€èªå‡¦ç†å­¦ä¼šç¬¬26å›å¹´æ¬¡å¤§ä¼š,
                            pp.1089-1092, 2020/03.</li>
                        <li>ä¼Šè—¤æ‹“æµ·, <u>æ —æ—æ¨¹ç”Ÿ</u>, è©åŸæ­£äºº, éˆ´æœ¨æ½¤, ä¹¾å¥å¤ªéƒ. è‹±èªè«–æ–‡åŸ·ç­†ã®ãŸã‚ã®çµ±åˆãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°æ”¯æ´ç’°å¢ƒ. ç¬¬14å›NLPè‹¥æ‰‹ã®ä¼š ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ  (YANS),
                            2019/08.</li>
                        <li>å°æ—æ‚Ÿéƒ, <u>æ —æ—æ¨¹ç”Ÿ</u>, æ¨ªäº•ç¥¥, éˆ´æœ¨æ½¤, ä¹¾å¥å¤ªéƒ. æ–‡è„ˆã‚’è€ƒæ…®ã™ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ãŒæ‰ãˆã‚‹å“è©æƒ…å ±ã¨ãã®è»Œè·¡. ç¬¬14å›NLPè‹¥æ‰‹ã®ä¼š ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ 
                            (YANS), 2019/08.</li>
                        <li><u>æ —æ—æ¨¹ç”Ÿ</u>, å¤§å†…å•“æ¨¹, äº•ä¹‹ç›´ä¹Ÿ, Paul Reisert, ä¸‰å¥½åˆ©æ˜‡, éˆ´æœ¨æ½¤, ä¹¾å¥å¤ªéƒ. 
                            è¤‡æ•°ã®è¨€èªå˜ä½ã«å¯¾ã™ã‚‹ã‚¹ãƒ‘ãƒ³è¡¨ç¾ã‚’ç”¨ã„ãŸè«–è¿°æ§‹é€ è§£æ. è¨€èªå‡¦ç†å­¦ä¼šç¬¬25å›å¹´æ¬¡å¤§ä¼š, pp.990-993, 2019/03.</li>
                        <li>*<u>æ —æ—æ¨¹ç”Ÿ</u>, *ä¼Šè—¤æ‹“æµ·, å†…å±±é¦™, éˆ´æœ¨æ½¤, ä¹¾å¥å¤ªéƒ. (* ç¬¬ä¸€è‘—è€…ã¨ç¬¬äºŒè‘—è€…ã®è²¢çŒ®åº¦ã¯ç­‰ã—ã„ï¼)
                             è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®èªé †è©•ä¾¡ã¨åŸºæœ¬èªé †ã®åˆ†æ. è¨€èªå‡¦ç†å­¦ä¼šç¬¬25å›å¹´æ¬¡å¤§ä¼š, pp.1053-1056, 2019/03.
                        </li>
                        <li>*ä¼Šè—¤æ‹“æµ·, *<u>æ —æ—æ¨¹ç”Ÿ</u>, å°æ—éš¼äºº, éˆ´æœ¨æ½¤, ä¹¾å¥å¤ªéƒ. (* ç¬¬ä¸€è‘—è€…ã¨ç¬¬äºŒè‘—è€…ã®è²¢çŒ®åº¦ã¯ç­‰ã—ã„ï¼)
                             ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°æ”¯æ´ã‚’æƒ³å®šã—ãŸæƒ…å ±è£œå®Œå‹ç”Ÿæˆ. è¨€èªå‡¦ç†å­¦ä¼šç¬¬25å›å¹´æ¬¡å¤§ä¼š, pp.970-973, 2019/03.
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Paul Reisert, Naoya Inoue and Kentaro Inui. Towards
                            Exploiting Argumentative Context for Argumentative Relation Identification. 
                            è¨€èªå‡¦ç†å­¦ä¼šç¬¬24å›å¹´æ¬¡å¤§ä¼š, pp.284-287, 2018/03.</li>
                        <li><u>Tatsuki Kuribayashi</u>, Paul Reisert, Naoya Inoue, Kentaro Inui. Examining
                            Macro-level Argumentative Structure Features for Argumentative Relation Identification.
                             ç¬¬4å›è‡ªç„¶è¨€èªå‡¦ç†ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ãƒ»ç¬¬234å›è‡ªç„¶è¨€èªå‡¦ç†ç ”ç©¶ä¼š, 6pages, 2017/12.
                        </li>
                    </ul>
                    <h2>Teaching Assistant</h2>
                    <ul>
                        <li>Oct.2018-Mar.2019: Advanced Creative Engineering Training (Step-Qi school)</li>
                    </ul>
                    <h2>Skills</h2>
                    <ul>
                        <li>Python (more than 6 years)</li>
                        <li>R, TypeScript, React (a litte...)</li>
                        <li>Tools in NLP/ML research: git, Docker, GCP, pytorch, w&b, TeX...</li>
                        <li>Designing API specifications for NLP models (with an experience of engineering adviser)</li>
                        <li>Developing evaluation/leader-board systems for NLP competition (with an experience of engineering adviser)</li>
                    </ul>
                    <h2>Hobbies</h2>
                    <ul>
                        <li>Saxophone (playing more than 10 years)</li>
                        <li>Skiing</li>
                        <li>Traveling</li>
                    </ul>
                </article>
            </div>
        </div>
        <footer>
        </footer>
    </div>
</body>

</html>
