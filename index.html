<!doctype html>
<html lang="">

<head>
    <meta charset="utf-8" />
    <title>Tatsuki Kuribayashi</title>
    <meta name="author" content="Tatsuki Kuribayashi">
    <link rel="top" href="#" />
    <link
        href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600,300italic,400italic,600italic|Source+Code+Pro'
        rel='stylesheet' type='text/css'>
    </link>
    <link rel="stylesheet" href="kuribayashi4.github.io/theme/css/main.css" type="text/css" />
    <link href="kuribayashi4.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
        title="Tatsuki Kuribayashi Atom Feed" />
</head>

<body>
    <div class="container">
        <div class="wrapper">
            <div role="main" class="content">
                <article>
                    <h1>Tatsuki Kuribayashi (栗林樹生)</h1>
                    <img src="./images/DSC01048.JPG" alt="写真" title="profile" width="264" height="180">
                    <ul>
                        <li>Ph.D. student (Advisor: <a href="http://www.cl.ecei.tohoku.ac.jp/~inui/">Prof. Kentaro
                                Inui</a>) </li>
                        <li>Tohoku NLP Lab (乾・鈴木研究室), Graduate School of Information Sciences, Tohoku University</li>
                        <li>Email: kuribayashi at tohoku.ac.jp </li>
                        <li>Address (office): Natural Language Processing Laboratory, 6-6-05 Aoba, Aramaki, Aoba-ku,
                            Sendai, 980-8579, Japan</li>
                        <li><a href="https://github.com/kuribayashi4">Github</a></li>
                        <li><a href="https://scholar.google.co.jp/citations?user=-bqmkaAAAAAJ&hl=ja">Google Scholar</a>
                        </li>
                        <li><a href="https://twitter.com/ttk_kuribayashi?lang=en">Twitter</a></li>
                    </ul>
                    <h2>Publications</h2>
                    <h3>(Refereed) Journal papers</h3>
                    <ul>
                        <li>Hiroki Ouchi, Junsuzuki, Sosuke Kobayahsi, Sho Yokoi, <u>Tatsuki Kuribayashi</u>, Masashi Yoshikawa, Kentaro Inui.<br>
                            "Instance-Based Neural Dependency Parsing"<br> Transactions of the Association for Computational Linguistics 2021 (TACL 2021), 2021/09<br>
                            [<a href="https://arxiv.org/abs/2109.13497">arXiv</a>|code]
                        </li>
                        <li><u>栗林樹生</u>, 大内啓樹, 井之上直也, 鈴木潤, Paul Reisert, 三好利昇, 乾健太郎<br>
                            「論述構造解析におけるスパン分散表現」<br>
                            自然言語処理 (domestic journal), Volume 27, Number 4, pp.753-780, December 2020<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jnlp/27/4/27_753/_article/-char/ja/">論文</a> |
                            <a href="./pdfs/20210308_Argmining_kuribayashi_public.pdf">年次大会招待講演資料</a>]
                        </li>
                        <li>Takumi Ito, <u>Tatsuki Kuribayashi</u>, Hayato Kobayashi, Ana Brassard, Masato Hagiwara, Jun
                            Suzuki, Kentaro Inui.<br>
                            "Assisting Authors to Convert Raw Products into Polished Prose."<br>
                            Journal of Cognitive Science, Vol.21, No.1, pp.99-135, 2020<br>
                            [paper]</li>
                    </ul>
                    <h3>(Refereed) International Conferences and Workshops</h3>
                    <ul>
                        <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                            "Incorporating Residual and Normalization Layers into Analysis of Masked Language
                            Models"<br> In proceedings of the 2021 Conference on Empirical Methods in Natural Language
                            Processing (EMNLP 2021), pp. 4547-4568, 2021/11 (acceptance rate: 23.3%)<br>
                            [<a href="https://aclanthology.org/2021.emnlp-main.373/">paper</a>|<a href="https://arxiv.org/abs/2109.07152">arXiv</a>|code]
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Yohei Oseki, Takumi Ito, Ryo Yoshida, Masayuki Asahara,
                            Kentaro Inui.<br>
                            "Lower Perplexity is Not Always Human-Like"<br>
                            In proceedings of the Joint Conference of the 59th Annual Meeting of the Association for
                            Computational Linguistics and the 11th International Joint Conference on Natural Language
                            Processing (ACL-IJCNLP 2021), pp. 5203-5217, 2021/08 (acceptance rate: 21.3%)<br>
                            [<a href="https://aclanthology.org/2021.acl-long.405/">paper</a> | <a
                                href="./pdfs/kuribayashi2021_revised.pdf">updated
                                version (fix some preprocessing errors)</a> | <a
                                href="https://github.com/kuribayashi4/surprisal_reading_time_en_ja">code</a>]
                        </li>
                        <li>Riki Fujihara, <u>Tatsuki Kuribayashi</u>, Kaori Abe, Kentaro Inui.<br>
                            "Topicalization in Language Models: A Case Study on Japanese"<br>
                            In proceedings of Student Research Workshop at the Joint Conference of the 59th Annual
                            Meeting of the Association for Computational Linguistics and the 11th International Joint
                            Conference on Natural Language Processing (ACL-IJCNLP-2021 SRW Non-archival), 2021/08
                            (acceptance rate: 39%)<br>
                        </li>
                        <li>Takaki Otake, Sho Yokoi, Naoya Inoue, Ryo Takahashi, <u>Tatsuki Kuribayashi</u>, Kentaro
                            Inui.<br>
                            "Modeling Event Salience in a Narrative Based on Barthes' Cardinal Function"<br>
                            In proceedings of the 28th International Conference on Computational Linguistics
                            (COLING-2020), pp. 1784-1794, 2020/12 (acceptance rate: 26.2%)<br>
                            [<a href="https://www.aclweb.org/anthology/2020.coling-main.160/">paper</a> | <a
                                href="https://arxiv.org/abs/2011.01785">arXiv</a>]
                        </li>
                        <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                            "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms"<br>
                            In proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
                            (EMNLP-2020), pp. 7057-7075, 2020/11 (acceptance rate: 754/3359=22.4%)<br>
                            [<a href="https://www.aclweb.org/anthology/2020.emnlp-main.574/">paper</a> | <a
                                href="https://arxiv.org/abs/2004.10102">arXiv</a> | <a
                                href="https://github.com/gorokoba560/norm-analysis-of-transformer">code</a>]
                        </li>
                        <li>*Takumi Ito, *<u>Tatsuki Kuribayashi</u>, *Masatoshi Hidaka, Jun Suzuki, Kentaro Inui. (*
                            equal contribution)<br>
                            "Langsmith: An Interactive Academic Text Revision System"<br>
                            In proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
                            (EMNLP-2020) (accepted to system demonstration track), pp. 216-226, 2020/11 (acceptance
                            rate: ???%)<br>
                            [<a href="https://www.aclweb.org/anthology/2020.emnlp-demos.28/">paper</a> | <a
                                href="https://arxiv.org/abs/2010.04332">arXiv</a> | <a
                                href="https://emnlp-demo.editor.langsmith.co.jp/">demo</a>]
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Takumi Ito, Jun Suzuki, Kentaro Inui.<br>
                            "Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in
                            Japanese"<br>
                            In proceedings of the 58th annual meeting of the Association for Computational Linguistics
                            (ACL-2020), pp. 6452-6459, 2020/07 (acceptance rate: 779/3429=22.7%)<br>
                            [<a href="https://www.aclweb.org/anthology/2020.acl-main.47/">paper</a> | <a
                                href="./pdfs/revised_20211223.pdf">updated version (correct desciptions)</a> | <a
                                href="https://github.com/kuribayashi4/LM_as_Word_Order_Evaluator">data</a> | <a
                                href="./pdfs/20200606_ACL2020.pptx">presentation source</a>]
                        </li>
                        <li>Hiroki Ouchi, Jun Suzuki, Sosuke Kobayashi, Sho Yokoi, <u>Tatsuki Kuribayashi</u>, Ryuto
                            Konno, Kentaro Inui.<br>
                            "Instance-Based Learning of Span Representations: A Case Study through Named Entity
                            Recognition"<br>
                            In proceedings of the 58th annual meeting of the Association for Computational Linguistics
                            (ACL-2020), pp. 6452-6459, 2020/07 (acceptance rate: 208/1185=17.6%)<br>
                            [<a href="https://www.aclweb.org/anthology/2020.acl-main.575/">paper</a> | <a
                                href="https://arxiv.org/abs/2004.14514">arXiv</a>]
                        </li>
                        <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                            "Self-Attention is Not Only a Weight: Analyzing BERT with Vector Norms"<br>
                            In proceedings of Student Research Workshop at the 58th Annual Meeting of the Association
                            for Computational Linguistics (ACL-SRW-2020 Non-archival), 2020/07 (acceptance rate:
                            72/202=35.6%)<br>
                            [<a href="https://arxiv.org/abs/2004.10102">arXiv (updated version)</a>]
                        </li>
                        <li>*Takumi Ito, *<u>Tatsuki Kuribayashi</u>, Hayato Kobayashi, Ana Brassard, Masato Hagiwara,
                            Jun Suzuki, Kentaro Inui. (* equal contribution)<br>
                            "Diamonds in the Rough: Generating Fluent Sentences from Early-stage Drafts for Academic
                            Writing Assistance". <br>
                            In Proceedings of the 12th International Conference on Natural Language Generation
                            (INLG-2019), pp. 40-53, 2019/10
                            (acceptance rate: 73/143=51.0%)<br>[<a
                                href="https://www.aclweb.org/anthology/W19-8606/">paper</a> | <a
                                href="https://arxiv.org/abs/1910.09180">arXiv</a> | <a
                                href="https://github.com/taku-ito/INLG2019_SentRev">data</a>]</li>

                        <li>Masato Hagiwara, Takumi Ito, <u>Tatsuki Kuribayashi</u>, Jun Suzuki, Kentaro Inui.<br>
                            "TEASPN: Framework and Protocol for Integrated Writing Assistance Environments". <br>
                            In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
                            (EMNLP-IJCNLP 2019) (accepted to system demonstrations track), pp. 229-234, 2019/11.
                            (acceptance rate: ???%)<br>[<a href="https://www.aclweb.org/anthology/D19-3039">paper</a>|
                            <a href="https://arxiv.org/abs/1909.02621">arXiv</a> | <a
                                href="https://github.com/teaspn/teaspn-sdk">code</a>]
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Hiroki Ouchi, Naoya Inoue, Paul Reisert, Toshinori Miyoshi, Jun
                            Suzuki, Kentaro Inui.<br>
                            "An Empirical Study of Span Representations in Argumentation Structure Parsing". <br>
                            In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics
                            (ACL-2019), pp. 4691-4698, 2019/07.
                            (acceptance rate: 213/1163=18.2%)<br>[<a
                                href="https://www.aclweb.org/anthology/P19-1464">paper</a> | <a
                                href="https://github.com/kuribayashi4/span_based_argumentation_parser">code</a>]</li>

                        <li>Paul Reisert, Naoya Inoue, <u>Tatsuki Kuribayashi</u>, Kentaro Inui.<br>
                            "Feasible Annotation Scheme for Capturing Policy Argument Reasoning using Argument
                            Templates".<br>
                            In Proceedings of the 5th Workshop on Argument Mining, pp.79-89, 2018/11.
                            (acceptance rate: 18/32=56.3%)<br>[<a
                                href="https://www.aclweb.org/anthology/W18-5210">paper</a> | <a
                                href="https://github.com/peldszus/arg-microtexts">data</a>]</li>
                    </ul>

                    <h3>Domestic Conferences</h3>
                    <ul>
                        <li>石月由紀子, <u>栗林樹生</u>, 松林優一郎, 大関洋平 <br>情報量に基づく日本語項省略の分析. <br>NLP若手の会 (YANS) 第16回シンポジウム,
                            2021/08.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥，乾健太郎. <br>非線形モジュールの加法分解に基づくTransformerレイヤーの解析. <br>NLP若手の会 (YANS)
                            第16回シンポジウム, 2021/08.</li>

                        </li>
                        <li><u>栗林樹生</u>, 大関洋平, 伊藤拓海, 吉田遼，浅原正幸, 乾健太郎.<br>予測の正確な言語モデルがヒトらしいとは限らない.<br>言語処理学会第27回年次大会,
                            pp.267-272,
                            2021/03.</li>
                        <li><u>栗林樹生</u>, 大関洋平, 伊藤拓海, 吉田遼，浅原正幸, 乾健太郎.<br>日本語の読みやすさに対する情報量に基づいた統一的な解釈.<br>言語処理学会第27回年次大会,
                            pp.723-728, 2021/03.</li>
                        <li>伊藤拓海, <u>栗林樹生</u>, 日高雅俊, 鈴木潤, 乾健太郎.<br>Langsmith: 人とシステムの協働による論文執筆.<br>言語処理学会第27回年次大会,
                            pp.1834-1839,
                            2021/03.</li>
                        <li>藤原吏生, <u>栗林樹生</u>, 乾健太郎.<br>人と言語モデルが捉える文の主題.<br>言語処理学会第27回年次大会, pp.1307-1312, 2021/03.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 乾健太郎.<br>Transformerの文脈を混ぜる作用と混ぜない作用.<br>言語処理学会第27回年次大会,
                            pp.1224-1229,
                            2021/03.</li>
                        <li>大内啓樹, 鈴木潤, 小林颯介, 横井祥, <u>栗林樹生</u>, 吉川将司,
                            乾健太郎.<br>事例ベース依存構造解析のための依存関係表現学習.<br>言語処理学会第27回年次大会, pp.497-502, 2021/03.</li>
                        <li>大竹孝樹, 横井祥, 井之上直也, 高橋諒, <u>栗林樹生</u>,
                            乾健太郎.<br>物語におけるイベントの顕現性推定と物語類似性計算への応用.<br>言語処理学会第27回年次大会, pp.1324-1329, 2021/03.</li>
                        <li>藤原吏生, <u>栗林樹生</u>, 乾健太郎.<br>日本語言語モデルが選択する文形式の傾向と文脈の影響 ー 主題化・有標語順について.<br>NLP若手の会 (YANS)
                            第15回シンポジウム, 2020/09.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 乾健太郎.<br>ベクトル長に基づく注意機構と残差結合の包括的な分析.<br>NLP若手の会 (YANS) 第15回シンポジウム,
                            2020/09.</li>
                        <li><u>栗林樹生</u>, 伊藤拓海, 鈴木潤, 乾健太郎.<br> 日本語語順分析に言語モデルを用いることの妥当性について.<br>言語処理学会第26回年次大会,
                            pp.493-496,
                            2020/03.<br>[<a href="./pdfs/nlp2020.pdf">slides</a>]</li>
                        <li>小林 悟郎, <u>栗林樹生</u>, 横井 祥, 鈴木潤, 乾健太郎.<br> ベクトル⻑に基づく自己注意機構の解析.<br>言語処理学会第26回年次大会, pp.965-968,
                            2020/03.</li>
                        <li>大内 啓樹, 鈴木 潤, 小林 颯介, 横井 祥, <u>栗林樹生</u>, 乾健太郎.<br> スパン間の類似性に基づく事例ベース構造予測.<br>言語処理学会第26回年次大会,
                            pp.331-334, 2020/03.</li>
                        <li>大竹 孝樹, 横井 祥, 井之上 直也, 高橋 諒, <u>栗林樹生</u>, 乾健太郎.<br> 言語モデルによる物語中のイベントの顕現性推定.<br>言語処理学会第26回年次大会,
                            pp.1089-1092, 2020/03.</li>
                        <li>伊藤拓海, <u>栗林樹生</u>, 萩原正人, 鈴木潤, 乾健太郎.<br>英語論文執筆のための統合ライティング支援環境.<br>第14回NLP若手の会 シンポジウム (YANS),
                            2019/08.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 鈴木潤, 乾健太郎.<br> 文脈を考慮する言語モデルが捉える品詞情報とその軌跡.<br>第14回NLP若手の会 シンポジウム
                            (YANS), 2019/08.</li>
                        <li><u>栗林樹生</u>, 大内啓樹, 井之直也, Paul Reisert, 三好利昇, 鈴木潤, 乾健太郎.<br>
                            複数の言語単位に対するスパン表現を用いた論述構造解析.<br>言語処理学会第25回年次大会, pp.990-993, 2019/03.</li>
                        <li>*<u>栗林樹生</u>, *伊藤拓海, 内山香, 鈴木潤, 乾健太郎. (* 第一著者と第二著者の貢献度は等しい．)
                            <br>言語モデルを用いた日本語の語順評価と基本語順の分析.<br>言語処理学会第25回年次大会, pp.1053-1056, 2019/03.
                        </li>
                        <li>*伊藤拓海, *<u>栗林樹生</u>, 小林隼人, 鈴木潤, 乾健太郎. (* 第一著者と第二著者の貢献度は等しい．)
                            <br>ライティング支援を想定した情報補完型生成.<br>言語処理学会第25回年次大会, pp.970-973, 2019/03.
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Paul Reisert, Naoya Inoue and Kentaro Inui. <br>Towards
                            Exploiting Argumentative Context for Argumentative Relation Identification.<br>
                            言語処理学会第24回年次大会, pp.284-287, 2018/03.</li>
                        <li><u>Tatsuki Kuribayashi</u>, Paul Reisert, Naoya Inoue, Kentaro Inui. <br>Examining
                            Macro-level Argumentative Structure Features for Argumentative Relation Identification.
                            <br>第4回自然言語処理シンポジウム・第234回自然言語処理研究会, 6pages, 2017/12.
                        </li>
                    </ul>
                    <h3>Others</h3>
                    <ul>
                        <li><u>栗林樹生</u>. Lower Perplexity is Not Always Human-Like (Talk). NLPコロキウム, June 2021<br>[<a
                                href="https://youtu.be/Xd_KfgWVWsI">動画</a>]</li>
                        <li><u>栗林樹生</u>. 学会記事「論述構造解析におけるスパン分散表現」の研究を通して. 自然言語処理 (学会記事No.27), Volume 28, Number 2,
                            pp.677-681, 2021/06</li>
                        <li><u>栗林樹生</u>. 学会記事 Language Models as an Alternative Evaluator of Word Order Hypotheses: A
                            Case Study in Japanese. 自然言語処理 (学会記事), Volume 27, Number 3, pp.671-676, 2020/09.
                        </li>
                    </ul>
                    <h2>Tools</h2>
                    <ul>
                        <li>Langsmith Editor<br>[<a href="https://editor.langsmith.co.jp/">system</a> | <a
                                href="https://help.editor.langsmith.co.jp/">usage</a>] </li>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/xgzxLpcydG8" frameborder="0"
                            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen></iframe>
                        <li>TEASPN (Text Editing Assistance Smartness Protocol for Natural Language)<br>[<a
                                href="https://github.com/teaspn/teaspn-sdk">code</a> | <a
                                href="https://www.teaspn.org/">document</a>] </li>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/9DNqpudDehM" frameborder="0"
                            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen></iframe>
                    </ul>
                    <h2>Awards</h2>
                    <ul>
                        <li><a href="https://www.anlp.jp/award/ronbun.html">Mar.2021: Association for Natural Language
                                Processing (domestic journal) Best Paper Award 2020 (言語処理学会2020年度最優秀論文賞) (1/28=3.5%)</a>
                            「論述構造解析におけるスパン分散表現」<br>[<a
                                href="pdfs/20210308_Argmining_kuribayashi_public.pdf">年次大会招待講演資料</a>]</li>
                        <li><a href="https://www.anlp.jp/nlp2021/">Mar. 2021: Special Committee Award at NLP2021
                                (domestic conference) (言語処理学会第27回年次大会，委員特別賞) (14/361=3.8%)</a>
                            「予測の正確な言語モデルがヒトらしいとは限らない」</li>
                        <li><a href="https://www.nlp.ecei.tohoku.ac.jp/news-release/3571/">Mar.2020: the Excellent
                                Student in Electrical and Information Science Award (電気・情報系優秀学生賞 受賞) </a></li>
                        <li><a href="https://www.anlp.jp/award/nenji.html">Mar.2020: Best Paper Award at NLP2020
                                (domestic conference) (言語処理学会年次大会最優秀賞受賞) (2/396=0.5%)</a> ベクトル⻑に基づく自己注意機構の解析. </li>
                        <li><a href="https://www.anlp.jp/award/nenji.html#y2019">Mar.2019: Young Researcher Award at
                                NLP2019 (domestic conference) (言語処理学会年次大会若手奨励賞受賞) (11/274=4.0%)</a>
                            言語モデルを用いた日本語の語順評価と基本語順の分析.</li>
                        <li><a href="http://www.ecei.tohoku.ac.jp/eipe/intro/news/commendation.html">Mar.2018: Excellent
                                Academic Award, School of Engineering, Tohoku University (東北大学工学部長賞受賞) (26/910=2.9%)</a>
                        </li>
                    </ul>
                    <h2>Education</h2>
                    <ul>
                        <li>Apr.2020-Present.: PhD student of Information Science, Graduate School of Information
                            Sciences, Tohoku University, Miyagi, Japan (Advisor: <a
                                href="http://www.cl.ecei.tohoku.ac.jp/~inui/">Prof. Kentaro Inui</a>) </li>
                        <li>Apr.2018-Mar.2020: Master's student of Information Science, Graduate School of Information
                            Sciences, Tohoku University, Miyagi, Japan (Advisor: <a
                                href="http://www.cl.ecei.tohoku.ac.jp/~inui/">Prof. Kentaro Inui</a>) </li>
                        <li>Apr.2018-Present.: Graduate Program in Data Science, Tohoku University.</li>
                        <li>Apr.2014-Mar.2018: Bachelor of Engineering, Department of Information and Intelligent
                            Systems, Tohoku University, Miyagi, Japan (Advisor: <a
                                href="http://www.cl.ecei.tohoku.ac.jp/~inui/">Prof. Kentaro Inui</a>) </li>
                    </ul>
                    <h2>Experience</h2>
                    <ul>
                        <li>April.2020-March.2023: Research fellow: 日本学術振興会特別研究員(DC1) (面接免除内定，54/277=19.5%)
                            "テクストの数理的モデリングと、数理モデルを通したテクストらしさの解明への挑戦"</li>
                        <li><a href="https://langsmith.co.jp/">May.2018-Present.: Langsmith Co., Ltd. (共同創業者，技術顧問)</a>
                        </li>
                        <li><a href="https://www.uni-goettingen.de/en/data+science/594011.html">Aug.2019: 3rd Data
                                Science Summer School in Göttingen</a></li>
                        <li><a href="https://internship.cookpad.com/2017/summer/17day-tech/">Aug.2017: Cookpad 17days
                                Internship</a></li>
                    </ul>
                    <h2>Activities</h2>
                    <ul>
                        <li>
                            <a href="https://sites.google.com/view/snlp-jp/home/2021">最先端NLP勉強会</a>, 2021 [<a
                                href="./pdfs/snlp2021.pdf">発表</a>]
                        </li>
                        <li>
                            <a href="https://sites.google.com/view/snlp-jp/home/2020">最先端NLP勉強会</a>, 2020 [<a
                                href="./pdfs/snlp2020.pdf">発表</a>]
                        </li>
                    </ul>
                    <h2>Organizer</h2>
                    <ul>
                        <li>
                            <a href="https://sites.google.com/view/snlp-jp/home/2021">最先端NLP勉強会</a>, 2021
                        </li>
                        <li>
                            <a href="https://sites.google.com/view/snlp-jp/home/2020">最先端NLP勉強会</a>, 2020
                        </li>
                    </ul>
                    <h2>Reviewer</h2>
                    <ul>
                        <li>EMNLP 2021 (Linguistic Theories, Cognitive Modeling and Psycholinguistics. Sentiment
                            Analysis, Stylistic Analysis, and Argument Mining)</li>
                        <li>ACL 2021 (Sentiment Analysis, Stylistic Analysis, and Argument Mining)</li>
                        <li>secondary: AAAI 2021</li>
                        <li>INLG 2020</li>
                        <li>COLING 2020</li>
                        <li>Secondary: ACL 2020</li>
                        <li>Secondary: ACL 2019</li>
                    </ul>
                    <h2>Teaching Assistant</h2>
                    <ul>
                        <li>Oct.2018-Mar.2019: Advanced Creative Engineering Training (Step-Qi school)</li>
                    </ul>
                    <h2>Programming languages</h2>
                    <ul>
                        <li>Python (5 years)</li>
                    </ul>
                    <h2>Interests</h2>
                    <ul>
                        <li>Computational psycholinguistics</li>
                        <li>Discourse processing</li>
                        <li>Argumentation mining</li>
                        <li>Automatic writing assistance</li>
                        <li>Anything could be interesting when one intensively explores it</li>
                    </ul>
                    <h2>Hobbies</h2>
                    <ul>
                        <li>Playing the saxophone (around 10 years)</li>
                        <li>Skiing</li>
                    </ul>
                </article>
            </div>
        </div>
        <footer>
        </footer>
    </div>
</body>

</html>
