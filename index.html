<!doctype html>
<html lang="">

<head>
    <meta charset="utf-8" />
    <title>Tatsuki Kuribayashi</title>
    <meta name="author" content="Tatsuki Kuribayashi">
    <link rel="top" href="#" />
    <link
        href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600,300italic,400italic,600italic|Source+Code+Pro'
        rel='stylesheet' type='text/css'>
    </link>
    <link rel="stylesheet" href="kuribayashi4.github.io/theme/css/main.css" type="text/css" />
    <link href="kuribayashi4.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
        title="Tatsuki Kuribayashi Atom Feed" />
</head>

<body>
    <div class="container">
        <div class="wrapper">
            <div role="main" class="content">
                <article style="padding: 2%">
                    <h1>Tatsuki Kuribayashi (栗林樹生)</h1>
                    <img src="./images/DSC_2965.JPG" alt="写真" title="profile" width="230" height="153">
                    <ul>
                        <li>Postdoctoral fellow </li>
                        <li>Tohoku NLP Lab (乾研究室), Graduate School of Information Sciences, Tohoku University</li>
                        <li>Email: kuribayashi at tohoku.ac.jp </li>
                        <li>Address (office): Natural Language Processing Laboratory, 6-6-05 Aoba, Aramaki, Aoba-ku,
                            Sendai, 980-8579, Japan</li>
                        <li><a href="https://github.com/kuribayashi4">Github</a></li>
                        <li><a href="https://scholar.google.co.jp/citations?user=-bqmkaAAAAAJ&hl=ja">Google Scholar</a>
                        </li>
                        <li><a href="https://twitter.com/ttk_kuribayashi?lang=en">Twitter</a></li>
                        <li><a href="pdfs/CV.pdf">CV (English)</a></li>
                    </ul>
                    <div style="width: 80%; padding: 1% 20% 0% 2%; color: gray; font-size: small;">
                    My research interest lies in the cognitive plausibility of NLP models---how does language processing in machines and humans differ? 
This involves a bunch of scientific questions: Can human language acquisition be achieved from (textual or beyond) data alone? What computations are performed during human language processing? Could these be quantified by NLP techniques?
<br/><br/>
Along with these, I am also interested in helping humans use/learn a language with NLP techniques.
Since language is for human-to-human communication, practicing language activities (e.g., writing/speaking, discussing, decision-making...) requires a human counterpart. 
I hope that a human-like computational model would be a beneficial partner for this purpose.
                    </div>
                    <h2>Publications</h2>
                    <h3>Refereed papers</h3>
                    <ul>
                        <li>Keito Kudo, Yoichi Aoki, <u>Tatsuki Kuribayashi</u>, Ana Brassard, Masashi Yoshikawa, Keisuke Sakaguchi and Kentaro Inui. <br>
                            "Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?" <br>
                            Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2023 <b>Short</b>), 2023/05. (acceptance rate: ??)<br>
                            [paper | arXiv]
                        </li>
                        <li>Yoichi Aoki, Keito Kudo, <u>Tatsuki Kuribayashi</u>, Ana Brassard, Masashi Yoshikawa, Keisuke Sakaguchi and Kentaro Inui. <br>
                            "Empirical Investigation of Neural Symbolic Reasoning Strategies." <br>
                            Findings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2023 <b>Findings Short</b>), 2023/05. (acceptance rate: ??)<br>
                            [paper | arXiv]
                        </li>
                        <ul>
                            <li>Yoichi Aoki, Keito Kudo, <u>Tatsuki Kuribayashi</u>, Ana Brassard, Masashi Yoshikawa, Keisuke Sakaguchi and Kentaro Inui. <br>
                                "Empirical Investigation of Neural Symbolic Reasoning Strategies." <br>
                                Non-archival submission for the 2022 AACL-IJCNLP Student Research Workshop (AACL-IJCNLP SRW <b>Non-archival</b>, best paper award), 2022/11. (acceptance rate: ??)</li>
                        </ul>
                        <li><u>Tatsuki Kuribayashi</u>, Yohei Oseki, Ana Brassard, Kentaro Inui.<br>
                            "Context Limitations Make Neural Language Models More Human-Like."<br>
                            In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022, <b>Long</b>), pp.10421–10436, 2022/12. (acceptance rate: 829/4190=20%)<br>
                            [<a href="https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.712/">paper</a> | <a href="https://arxiv.org/abs/2205.11463">arXiv</a>]
                        </li>
                        <li>
                            Riki Fujihara, <u>Tatsuki Kuribayashi</u>, Kaori Abe, Ryoko Tokuhisa, Kentaro Inui.<br>
                            "Topicalization in Language Models: A Case Study on Japanese."<br> 
                            In Proceedings of the 29th International Conference on Computational Linguistics (COLING 2022, <b>Long</b>), pp.851-862, 2022/10 (acceptance rate: 522/1563=33.4%).<br>
                            [<a href="https://aclanthology.org/2022.coling-1.71/">paper</a> | arXiv]
                        </li>
                        <ul>
                            <li>Riki Fujihara, <u>Tatsuki Kuribayashi</u>, Kaori Abe, Kentaro Inui.<br>
                                "Topicalization in Language Models: A Case Study on Japanese."<br>
                                In proceedings of Student Research Workshop at the Joint Conference of the 59th Annual
                                Meeting of the Association for Computational Linguistics and the 11th International Joint
                                Conference on Natural Language Processing (ACL-IJCNLP-2021 SRW <b>Non-archival</b>), 2021/08
                                (acceptance rate: 39%).<br>
                            </li>
                        </ul>
                        <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                            "Incorporating Residual and Normalization Layers into Analysis of Masked Language
                            Models."<br> In proceedings of the 2021 Conference on Empirical Methods in Natural Language
                            Processing (EMNLP 2021, <b>Long</b>), pp. 4547-4568, 2021/11 (acceptance rate: 23.3%).<br>
                            [<a href="https://aclanthology.org/2021.emnlp-main.373/">paper</a> | <a href="https://arxiv.org/abs/2109.07152">arXiv</a> | code]
                        </li>
                        <li>Hiroki Ouchi, Jun Suzuki, Sosuke Kobayashi, Sho Yokoi, <u>Tatsuki Kuribayashi</u>, Masashi Yoshikawa, Kentaro Inui.<br>
                            "Instance-Based Neural Dependency Parsing."<br> Transactions of the Association for Computational Linguistics 2021 (TACL 2021), 2021/09.<br>
                            [<a href="https://aclanthology.org/2021.tacl-1.89/">paper</a> | <a href="https://arxiv.org/abs/2109.13497">arXiv</a>]
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Yohei Oseki, Takumi Ito, Ryo Yoshida, Masayuki Asahara,
                            Kentaro Inui.<br>
                            "Lower Perplexity is Not Always Human-Like."<br>
                            In proceedings of the Joint Conference of the 59th Annual Meeting of the Association for
                            Computational Linguistics and the 11th International Joint Conference on Natural Language
                            Processing (ACL-IJCNLP 2021, <b>Long</b>), pp. 5203-5217, 2021/08 (acceptance rate: 21.3%).<br>
                            [<a href="https://aclanthology.org/2021.acl-long.405/">paper (updated)</a> | <a
                                href="https://github.com/kuribayashi4/surprisal_reading_time_en_ja">code</a>]
                        </li>
                        <li><u>栗林樹生</u>, 大内啓樹, 井之上直也, 鈴木潤, Paul Reisert, 三好利昇, 乾健太郎<br>
                            「論述構造解析におけるスパン分散表現」<br>
                            自然言語処理 (domestic journal), Volume 27, Number 4, pp.753-780, December 2020.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jnlp/27/4/27_753/_article/-char/ja/">paper</a> |
                            <a href="./pdfs/20210308_Argmining_kuribayashi_public.pdf">slides</a>]
                        </li>
                        <li>Takaki Otake, Sho Yokoi, Naoya Inoue, Ryo Takahashi, <u>Tatsuki Kuribayashi</u>, Kentaro
                            Inui.<br>
                            "Modeling Event Salience in a Narrative Based on Barthes' Cardinal Function."<br>
                            In proceedings of the 28th International Conference on Computational Linguistics
                            (COLING-2020, <b>Short</b>), pp. 1784-1794, 2020/12 (acceptance rate: 26.2%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.coling-main.160/">paper</a> | <a
                                href="https://arxiv.org/abs/2011.01785">arXiv</a>]
                        </li>
                        <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                            "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms."<br>
                            In proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
                            (EMNLP-2020, <b>Long</b>), pp. 7057-7075, 2020/11 (acceptance rate: 754/3359=22.4%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.emnlp-main.574/">paper</a> | <a
                                href="https://arxiv.org/abs/2004.10102">arXiv</a> | <a
                                href="https://github.com/gorokoba560/norm-analysis-of-transformer">code</a>]
                        </li>
                        <ul>
                            <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                                "Self-Attention is Not Only a Weight: Analyzing BERT with Vector Norms."<br>
                                In proceedings of Student Research Workshop at the 58th Annual Meeting of the Association
                                for Computational Linguistics (ACL-SRW-2020 <b>Non-archival</b>), 2020/07 (acceptance rate:
                                72/202=35.6%).<br>
                                [<a href="https://arxiv.org/abs/2004.10102">arXiv (updated version)</a>]
                            </li>
                        </ul>
                        <li>*Takumi Ito, *<u>Tatsuki Kuribayashi</u>, *Masatoshi Hidaka, Jun Suzuki, Kentaro Inui. (*
                            equal contribution)<br>
                            "Langsmith: An Interactive Academic Text Revision System."<br>
                            In proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
                            (EMNLP-2020, <b>Demo</b>), pp. 216-226, 2020/11 (acceptance
                            rate: ???%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.emnlp-demos.28/">paper</a> | <a
                                href="https://arxiv.org/abs/2010.04332">arXiv</a> | <a
                                href="https://emnlp-demo.editor.langsmith.co.jp/">demo</a>]
                        </li>
                        <li>Takumi Ito, <u>Tatsuki Kuribayashi</u>, Hayato Kobayashi, Ana Brassard, Masato Hagiwara, Jun
                            Suzuki, Kentaro Inui.<br>
                            "Assisting Authors to Convert Raw Products into Polished Prose."<br>
                            Journal of Cognitive Science, Vol.21, No.1, pp.99-135, 2020.<br>
                            [paper]</li>
                        <li><u>Tatsuki Kuribayashi</u>, Takumi Ito, Jun Suzuki, Kentaro Inui.<br>
                            "Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in
                            Japanese."<br>
                            In proceedings of the 58th annual meeting of the Association for Computational Linguistics
                            (ACL-2020, <b>Long</b>), pp. 6452-6459, 2020/07 (acceptance rate: 779/3429=22.7%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.acl-main.47/">paper</a> | <a
                                href="./pdfs/revised_20211223.pdf">updated version (correct desciptions)</a> | <a
                                href="https://github.com/kuribayashi4/LM_as_Word_Order_Evaluator">data</a> | <a
                                href="./pdfs/20200606_ACL2020.pptx">presentation source</a>]
                        </li>
                        <li>Hiroki Ouchi, Jun Suzuki, Sosuke Kobayashi, Sho Yokoi, <u>Tatsuki Kuribayashi</u>, Ryuto
                            Konno, Kentaro Inui.<br>
                            "Instance-Based Learning of Span Representations: A Case Study through Named Entity
                            Recognition."<br>
                            In proceedings of the 58th annual meeting of the Association for Computational Linguistics
                            (ACL-2020, <b>Short</b>), pp. 6452-6459, 2020/07 (acceptance rate: 208/1185=17.6%).<br>
                            [<a href="https://www.aclweb.org/anthology/2020.acl-main.575/">paper</a> | <a
                                href="https://arxiv.org/abs/2004.14514">arXiv</a>]
                        </li>
                        <li>*Takumi Ito, *<u>Tatsuki Kuribayashi</u>, Hayato Kobayashi, Ana Brassard, Masato Hagiwara,
                            Jun Suzuki, Kentaro Inui. (* equal contribution)<br>
                            "Diamonds in the Rough: Generating Fluent Sentences from Early-stage Drafts for Academic
                            Writing Assistance." <br>
                            In Proceedings of the 12th International Conference on Natural Language Generation
                            (INLG-2019), pp. 40-53, 2019/10
                            (acceptance rate: 73/143=51.0%).<br>[<a
                                href="https://www.aclweb.org/anthology/W19-8606/">paper</a> | <a
                                href="https://arxiv.org/abs/1910.09180">arXiv</a> | <a
                                href="https://github.com/taku-ito/INLG2019_SentRev">data</a>]</li>

                        <li>Masato Hagiwara, Takumi Ito, <u>Tatsuki Kuribayashi</u>, Jun Suzuki, Kentaro Inui.<br>
                            "TEASPN: Framework and Protocol for Integrated Writing Assistance Environments." <br>
                            In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
                            (EMNLP-IJCNLP 2019, <b>Demo</b>), pp. 229-234, 2019/11 
                            (acceptance rate: ???%).<br>[<a href="https://www.aclweb.org/anthology/D19-3039">paper</a>|
                            <a href="https://arxiv.org/abs/1909.02621">arXiv</a> | <a
                                href="https://github.com/teaspn/teaspn-sdk">code</a>]
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Hiroki Ouchi, Naoya Inoue, Paul Reisert, Toshinori Miyoshi, Jun
                            Suzuki, Kentaro Inui.<br>
                            "An Empirical Study of Span Representations in Argumentation Structure Parsing." <br>
                            In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics
                            (ACL-2019, <b>Short</b>), pp. 4691-4698, 2019/07 
                            (acceptance rate: 213/1163=18.2%).<br>[<a
                                href="https://www.aclweb.org/anthology/P19-1464">paper</a> | <a
                                href="https://github.com/kuribayashi4/span_based_argumentation_parser">code</a>]</li>

                        <li>Paul Reisert, Naoya Inoue, <u>Tatsuki Kuribayashi</u>, Kentaro Inui.<br>
                            "Feasible Annotation Scheme for Capturing Policy Argument Reasoning using Argument
                            Templates." <br>
                            In Proceedings of the 5th Workshop on Argument Mining, pp.79-89, 2018/11 
                            (acceptance rate: 18/32=56.3%).<br>[<a
                                href="https://www.aclweb.org/anthology/W18-5210">paper</a> | <a
                                href="https://github.com/peldszus/arg-microtexts">data</a>]</li>
                    </ul>
                    <h3>Preprints</h3>
                    <ul>
                    <li><u>Tatsuki Kuribayashi</u>.<br>
                        "Does Vision Accelerate Hierarchical Generalization of Neural Language Learner?" <br>[<a
                            href="https://arxiv.org/abs/2302.00667">arXiv</a>]</li>
                        <li>Goro Kobayashi, <u>Tatsuki Kuribayashi</u>, Sho Yokoi, Kentaro Inui.<br>
                            "Feed-Forward Blocks Control Contextualization in Masked Language Models" <br>[<a
                                href="https://arxiv.org/abs/2302.00456">arXiv</a>]</li>
                    </ul>
                    <h2>Tools</h2>
                    <ul>
                        <li>Langsmith Editor<br>[<a href="https://editor.langsmith.co.jp/">system</a> | <a
                                href="https://help.editor.langsmith.co.jp/">usage</a>] </li>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/xgzxLpcydG8" frameborder="0"
                            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen></iframe>
                        <li>TEASPN (Text Editing Assistance Smartness Protocol for Natural Language)<br>[<a
                                href="https://github.com/teaspn/teaspn-sdk">code</a> | <a
                                href="https://www.teaspn.org/">document</a>] </li>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/9DNqpudDehM" frameborder="0"
                            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen></iframe>
                    </ul>
                    <h3>Talks/others</h3>
                    <ul>
                        <li>自然言語処理による論文執筆支援. 東北大学（大学院改革推進センター主催）, 2023/2.<br>
                            [<a href="https://speakerdeck.com/kuribayashi4/zi-ran-yan-yu-chu-li-niyorulun-wen-zhi-bi-zhi-yuan">slides</a>]
                        </li>
                        <li>
                            Talk at <a href="https://mbzuai.ac.ae/research/department/natural-language-processing-department/">MBZUAI</a>. "Cognitive Plausibility of Neual Language Models," 2022/12.<br>
                          [<a href="https://speakerdeck.com/kuribayashi4/cognitive-plausibility-of-neural-language-models">slides</a>]
                        </li>
                        <li>
                            <a href="https://sites.google.com/view/snlp-jp/home/2022">最先端NLP勉強会</a>, 2022/09. <br>
                            [<a
                                href="./pdfs/snlp.pdf">slides</a>]
                        </li>
                        <li>
                            <a href="https://sites.google.com/view/snlp-jp/home/2021">最先端NLP勉強会</a>, 2021/09. <br>
                            [<a
                                href="./pdfs/snlp2021.pdf">slides</a>]
                        </li>
                        <li><u>栗林樹生</u>. Lower Perplexity is Not Always Human-Like (Talk). NLPコロキウム, 2021/06.<br>[<a
                            href="https://youtu.be/Xd_KfgWVWsI">動画</a>]</li>
                        <li><u>栗林樹生</u>. 学会記事「論述構造解析におけるスパン分散表現」の研究を通して. 自然言語処理 (学会記事No.27), Volume 28, Number 2,
                                pp.677-681, 2021/06.</li>
                        <li><u>栗林樹生</u>. 学会記事 Language Models as an Alternative Evaluator of Word Order Hypotheses: A
                                    Case Study in Japanese. 自然言語処理 (学会記事), Volume 27, Number 3, pp.671-676, 2020/09.
                         </li>
                        <li>Joining NAIST NLP Parsing Hackathon, 2021/07.</li>
                        <li>
                            <a href="https://sites.google.com/view/snlp-jp/home/2020">最先端NLP勉強会</a>, 2020/09.<br>
                            [<a
                                href="./pdfs/snlp2020.pdf">slides</a>]
                        </li>
                    </ul>
                    <h2>Awards</h2>
                    <ul>
                        <li><a href="https://aacl2022-srw.github.io/program">2022/12: Excellent Research Award, 第254回 NL研 (domestic conference)</a>. 言語モデルの第二言語獲得効率.</li>
                        <li><a href="https://aacl2022-srw.github.io/program">2022/11: AACL-SRW Best Paper Award</a>. Empirical Investigation of Neural Symbolic Reasoning Strategies.</li>
                        <li><a href="https://yans.anlp.jp/entry/yans2022report">2022/08: Encouragement Awards (奨励賞), YANS2022 (domestic meeting).</a> 視覚情報は言語モデルに人間らしい統語的汎化を促すか.</li>
                        <li>2022/03: President's Award, Graduate School of Information Sciences, Tohoku University. (東北大学総長賞)</li>
                        <li><a href="https://note.com/ipsj/n/nefc2ae945988">2021年度研究会推薦博士論文</a>. Exploring Cognitive Plausibility of Neural NLP Models: Cross-Linguistic and Discourse-Level Studies.</li>
                        <li>2022/03: Excellent
                            Student in Electrical and Information Science Award, Tohoku University. (東北大学電気・情報系優秀学生賞)</li>
                        <li>2022/03: Special Committee Award, NLP2022 (domestic conference). (言語処理学会第28回年次大会, 委員特別賞) 「Transformerにおけるフィードフォワードネットの作用」</li>
                        <li><a href="https://www.anlp.jp/award/ronbun.html">2021/03: Best Paper Award 2020, Association for Natural Language
                                Processing (domestic journal). (言語処理学会2020年度最優秀論文賞) (1/28=3.5%)</a>
                            「論述構造解析におけるスパン分散表現」<br>[<a
                                href="pdfs/20210308_Argmining_kuribayashi_public.pdf">年次大会招待講演資料</a>]</li>
                        <li><a href="https://www.anlp.jp/nlp2021/">2021/03: Special Committee Award, NLP2021
                                (domestic conference). (言語処理学会第27回年次大会, 委員特別賞) (14/361=3.8%)</a>
                            「予測の正確な言語モデルがヒトらしいとは限らない」</li>
                        <li><a href="https://www.nlp.ecei.tohoku.ac.jp/news-release/3571/">2020/03: Excellent
                                Student in Electrical and Information Science Award, Tohoku University. (東北大学電気・情報系優秀学生賞) </a></li>
                        <li><a href="https://www.anlp.jp/award/nenji.html">2020/03: Best Paper Award, NLP2020
                                (domestic conference) (言語処理学会年次大会最優秀賞) (2/396=0.5%)</a> ベクトル⻑に基づく自己注意機構の解析. </li>
                        <li><a href="https://www.anlp.jp/award/nenji.html#y2019">2019/03: Young Researcher Award, 
                                NLP2019 (domestic conference). (言語処理学会年次大会若手奨励賞) (11/274=4.0%)</a>
                            言語モデルを用いた日本語の語順評価と基本語順の分析.</li>
                        <li><a href="http://www.ecei.tohoku.ac.jp/eipe/intro/news/commendation.html">2018/03: Excellent
                                Academic Award, School of Engineering, Tohoku University. (東北大学工学部長賞) (26/910=2.9%)</a>
                        </li>
                    </ul>
                    <h3>Domestic conferences</h3>
                    <ul>
                        <li><u>栗林樹生</u>. 百聞は一見に如かず？視覚情報は言語モデルに文の階層構造を教示するか. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 乾健太郎. Transformer言語モデルの予測ヘッド内バイアスによる頻度補正効果. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>葉夢宇, <u>栗林樹生</u>, 舟山弘晃, 鈴木潤. 思考連鎖指示における大規模言語モデルの否定表現理解. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>藤原吏生, <u>栗林樹生</u>, 徳久良子, 乾健太郎. 主題化における人間と言語モデルの対照. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>石月由紀子, <u>栗林樹生</u>, 松林優一郎, 笹野遼平, 乾健太郎. 日本語話者の項省略判断に関するアノテーションとモデリング. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>大羽未悠, <u>栗林樹生</u>, 大内啓樹, 渡辺太郎. 言語モデルの第二言語獲得. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>工藤慧音, 青木洋一, <u>栗林樹生</u>, Ana Brassard, 吉川将司, 坂口慶祐, 乾健太郎. 算術問題におけるニューラルモデルの構成的推論能力. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>青木洋一, 工藤慧音, Ana Brassard, <u>栗林樹生</u>, 吉川将司, 坂口慶祐, 乾健太郎. ニューラル記号推論における推論過程の教示方法. 言語処理学会第29回年次大会, 4pages, 2023/3.</li>
                        <li>大羽未悠, <u>栗林樹生</u>, 大内啓樹, 渡辺太郎. 言語モデルの第二言語獲得効率. 第254回自然言語処理研究会, 6pages, 2022/11.</li>
                        <li>青木洋一, 工藤慧音, <u>栗林樹生</u>, Ana Brassard, 吉川将司, 乾健太郎. 推論過程の性質がニューラルネットの多段推論能力に与える影響. 第17回NLP若手の会 シンポジウム (YANS), 2022/08.</li>
                        <li><u>栗林樹生</u>. 視覚情報は言語モデルに人間らしい統語的汎化を促すか. 第17回NLP若手の会 シンポジウム (YANS), 2022/08.</li>
                        <li>大羽未悠, <u>栗林樹生</u>, 大内啓樹, 渡辺太郎. 言語モデルの第二言語獲得効率. 第17回NLP若手の会 シンポジウム (YANS), 2022/08.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 乾健太郎. Transformerにおけるフィードフォワードネットの混ぜ合わせ作用. 第17回NLP若手の会 シンポジウム (YANS), 2022/08.</li>
                        <li><u>栗林樹生</u>, 大関洋平, Ana Brassard, 乾健太郎. ニューラル言語モデルの過剰な作業記憶. 言語処理学会第28回年次大会, pp.1530-1535, 2022/03.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 乾健太郎. Transformerにおけるフィードフォワードネットの作用. 言語処理学会第28回年次大会, pp.1072-1077, 2022/03.</li>
                        <li>石月由紀子, <u>栗林樹生</u>, 松林優一郎, 大関洋平. 情報量に基づく日本語項省略の分析. 言語処理学会第28回年次大会, pp.442-447, 2022/03.</li>
                        <li>青木洋一, 工藤慧音, Ana Brassard, <u>栗林樹生</u>, 吉川将司, 乾健太郎. 多段の数量推論タスクに対する適応的なモデルの振る舞いの検証. 言語処理学会第28回年次大会, pp.168-172, 2022/03.</li>
                        <li>石月由紀子, <u>栗林樹生</u>, 松林優一郎, 大関洋平 情報量に基づく日本語項省略の分析. NLP若手の会 (YANS) 第16回シンポジウム,
                            2021/08.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 乾健太郎. 非線形モジュールの加法分解に基づくTransformerレイヤーの解析. NLP若手の会 (YANS)
                            第16回シンポジウム, 2021/08.</li>
                        <li><u>栗林樹生</u>, 大関洋平, 伊藤拓海, 吉田遼, 浅原正幸, 乾健太郎. 予測の正確な言語モデルがヒトらしいとは限らない. 言語処理学会第27回年次大会,
                            pp.267-272,
                            2021/03.</li>
                        <li><u>栗林樹生</u>, 大関洋平, 伊藤拓海, 吉田遼, 浅原正幸, 乾健太郎. 日本語の読みやすさに対する情報量に基づいた統一的な解釈. 言語処理学会第27回年次大会,
                            pp.723-728, 2021/03.</li>
                        <li>伊藤拓海, <u>栗林樹生</u>, 日高雅俊, 鈴木潤, 乾健太郎. Langsmith: 人とシステムの協働による論文執筆. 言語処理学会第27回年次大会,
                            pp.1834-1839,
                            2021/03.</li>
                        <li>藤原吏生, <u>栗林樹生</u>, 乾健太郎. 人と言語モデルが捉える文の主題. 言語処理学会第27回年次大会, pp.1307-1312, 2021/03.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 乾健太郎. Transformerの文脈を混ぜる作用と混ぜない作用. 言語処理学会第27回年次大会,
                            pp.1224-1229,
                            2021/03.</li>
                        <li>大内啓樹, 鈴木潤, 小林颯介, 横井祥, <u>栗林樹生</u>, 吉川将司,
                            乾健太郎. 事例ベース依存構造解析のための依存関係表現学習. 言語処理学会第27回年次大会, pp.497-502, 2021/03.</li>
                        <li>大竹孝樹, 横井祥, 井之上直也, 高橋諒, <u>栗林樹生</u>,
                            乾健太郎. 物語におけるイベントの顕現性推定と物語類似性計算への応用. 言語処理学会第27回年次大会, pp.1324-1329, 2021/03.</li>
                        <li>藤原吏生, <u>栗林樹生</u>, 乾健太郎. 日本語言語モデルが選択する文形式の傾向と文脈の影響 ー 主題化・有標語順について. NLP若手の会 (YANS)
                            第15回シンポジウム, 2020/09.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 乾健太郎. ベクトル長に基づく注意機構と残差結合の包括的な分析. NLP若手の会 (YANS) 第15回シンポジウム,
                            2020/09.</li>
                        <li><u>栗林樹生</u>, 伊藤拓海, 鈴木潤, 乾健太郎. 日本語語順分析に言語モデルを用いることの妥当性について. 言語処理学会第26回年次大会,
                            pp.493-496,
                            2020/03.<br>[<a href="./pdfs/nlp2020.pdf">slides</a>]</li>
                        <li>小林 悟郎, <u>栗林樹生</u>, 横井 祥, 鈴木潤, 乾健太郎. ベクトル⻑に基づく自己注意機構の解析. 言語処理学会第26回年次大会, pp.965-968,
                            2020/03.</li>
                        <li>大内 啓樹, 鈴木 潤, 小林 颯介, 横井 祥, <u>栗林樹生</u>, 乾健太郎. スパン間の類似性に基づく事例ベース構造予測. 言語処理学会第26回年次大会,
                            pp.331-334, 2020/03.</li>
                        <li>大竹 孝樹, 横井 祥, 井之上 直也, 高橋 諒, <u>栗林樹生</u>, 乾健太郎. 言語モデルによる物語中のイベントの顕現性推定. 言語処理学会第26回年次大会,
                            pp.1089-1092, 2020/03.</li>
                        <li>伊藤拓海, <u>栗林樹生</u>, 萩原正人, 鈴木潤, 乾健太郎. 英語論文執筆のための統合ライティング支援環境. 第14回NLP若手の会 シンポジウム (YANS),
                            2019/08.</li>
                        <li>小林悟郎, <u>栗林樹生</u>, 横井祥, 鈴木潤, 乾健太郎. 文脈を考慮する言語モデルが捉える品詞情報とその軌跡. 第14回NLP若手の会 シンポジウム
                            (YANS), 2019/08.</li>
                        <li><u>栗林樹生</u>, 大内啓樹, 井之直也, Paul Reisert, 三好利昇, 鈴木潤, 乾健太郎. 
                            複数の言語単位に対するスパン表現を用いた論述構造解析. 言語処理学会第25回年次大会, pp.990-993, 2019/03.</li>
                        <li>*<u>栗林樹生</u>, *伊藤拓海, 内山香, 鈴木潤, 乾健太郎. (* 第一著者と第二著者の貢献度は等しい．)
                             言語モデルを用いた日本語の語順評価と基本語順の分析. 言語処理学会第25回年次大会, pp.1053-1056, 2019/03.
                        </li>
                        <li>*伊藤拓海, *<u>栗林樹生</u>, 小林隼人, 鈴木潤, 乾健太郎. (* 第一著者と第二著者の貢献度は等しい．)
                             ライティング支援を想定した情報補完型生成. 言語処理学会第25回年次大会, pp.970-973, 2019/03.
                        </li>
                        <li><u>Tatsuki Kuribayashi</u>, Paul Reisert, Naoya Inoue and Kentaro Inui. Towards
                            Exploiting Argumentative Context for Argumentative Relation Identification. 
                            言語処理学会第24回年次大会, pp.284-287, 2018/03.</li>
                        <li><u>Tatsuki Kuribayashi</u>, Paul Reisert, Naoya Inoue, Kentaro Inui. Examining
                            Macro-level Argumentative Structure Features for Argumentative Relation Identification.
                             第4回自然言語処理シンポジウム・第234回自然言語処理研究会, 6pages, 2017/12.
                        </li>
                    </ul>
                    <h2>Education</h2>
                    <ul> 
                        <li>2020/04-2022/03: PhD student of Information Science, Graduate School of Information
                            Sciences, Tohoku University, Miyagi, Japan. (Advisor: <a
                                href="http://www.cl.ecei.tohoku.ac.jp/~inui/">Prof. Kentaro Inui</a>) <br>
                                [<a href="https://www.cl.ecei.tohoku.ac.jp/local/handouts/2021/thesis/thesis-20220218-kuribayashi.pdf">thesis</a>]</li>
                        <li>2018/04-2020/03: Master's student of Information Science, Graduate School of Information
                            Sciences, Tohoku University, Miyagi, Japan. (Advisor: <a
                                href="http://www.cl.ecei.tohoku.ac.jp/~inui/">Prof. Kentaro Inui</a>) <br>
                            [<a href="http://www.cl.ecei.tohoku.ac.jp/local/handouts/2019/thesis/thesis-20200401-kuribayashi.pdf">thesis</a>]</li>
                        <li>2018/04-2022/03.: Graduate Program in Data Science, Tohoku University.</li>
                        <li>2014/04-2018/03: Bachelor of Engineering, Department of Information and Intelligent
                            Systems, Tohoku University, Miyagi, Japan. (Advisor: <a
                                href="http://www.cl.ecei.tohoku.ac.jp/~inui/">Prof. Kentaro Inui</a>) </li>
                    </ul>
                    <h2>Experience</h2>
                    <ul>
                        <li>2020/04-2022/03: Research fellow: 日本学術振興会特別研究員(DC1) (面接免除内定, 54/277=19.5%).
                            「テクストの数理的モデリングと、数理モデルを通したテクストらしさの解明への挑戦」</li>
                        <li><a href="https://langsmith.co.jp/">2018/05-Present.: Langsmith Co., Ltd. (共同創業者)</a>.
                        </li>
                        <li><a href="https://www.uni-goettingen.de/en/data+science/594011.html">2019/08: 3rd Data
                                Science Summer School in Göttingen</a>.</li>
                        <li><a href="https://internship.cookpad.com/2017/summer/17day-tech/">2017/08: Cookpad 17days
                                Internship</a>.</li>
                    </ul>
                    <h2>Organizer/activities</h2>
                    <ul>
                        <li><a href="https://cmclorg.github.io/">CMCL</a> organizer, 2023- (if workshop is accepted)</li>
                        <li>
                            <a href="https://sites.google.com/view/snlp-jp/home/2022">最先端NLP勉強会</a>, 2022.
                        </li>
                        <li>
                            <a href="https://sites.google.com/view/snlp-jp/home/2021">最先端NLP勉強会</a>, 2021.
                        </li>
                        <li>
                            <a href="https://sites.google.com/view/snlp-jp/home/2020">最先端NLP勉強会</a>, 2020.
                        </li>
                        <li>Member of Computational Psycholinguistics Tokyo.</li>
                        <li>Member of NLP Dの会, 2021-2022.</li>
                    </ul>
                    <h2>Reviewer</h2>
                    <ul>
                        <li>ACL: 2019 (secondary), 2020 (secondary), 2021 (Sentiment Analysis, Stylistic Analysis, and Argument Mining).</li>
                        <li>EMNLP: 2021 (Linguistic Theories, Cognitive Modeling and Psycholinguistics. Sentiment
                            Analysis, Stylistic Analysis, and Argument Mining), 2022 (Linguistic Theories, Cognitive Modeling and Psycholinguistics).</li>
                        <li>COLING: 2020, 2022 (Language Modeling. Integrated Systems and Applications).</li>
                        <li>LREC: 2022 (Language Resources and Evaluation for Psycholinguistics, Cognitive Linguistics and Linguistic Theories. Natural Language Generation (including Summarization)).</li>
                        <li>INLG: 2020, 2021.</li>
                        <li>AAAI: 2021 (secondary).</li>
                        <li>ACL Rolling Review.</li>
                        <li>言語処理学会年次大会 (domestic conference), 第27回, 第28回, 第29回.</li>
                    </ul>
                    <h2>Teaching Assistant</h2>
                    <ul>
                        <li>Oct.2018-Mar.2019: Advanced Creative Engineering Training (Step-Qi school)</li>
                    </ul>
                    <h2>Skills</h2>
                    <ul>
                        <li>Python (6 years)</li>
                        <li>R (a little)</li>
                        <li>TypeScript, React (a litte...)</li>
                        <li>Tools in NLP/ML research: git, Docker, GCP, pytorch, w&b, TeX... (still not super-expert)</li>
                        <li>Designing API specifications for NLP models (working as a subcontractor)</li>
                        <li>Developing evaluation systems for NLP competition (working as a subcontractor)</li>
                    </ul>
                    <h2>Hobbies</h2>
                    <ul>
                        <li>Playing the saxophone (around 10 years)</li>
                        <li>Skiing</li>
                    </ul>
                </article>
            </div>
        </div>
        <footer>
        </footer>
    </div>
</body>

</html>
